{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë¼ì´ë¸ŒëŸ¬ë¦¬, API ì ‘ì†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.1.20-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.11/site-packages (from langchain) (2.0.22)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Downloading aiohttp-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Downloading dataclasses_json-0.6.6-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.38 (from langchain)\n",
      "  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in /opt/conda/lib/python3.11/site-packages (from langchain) (0.1.52)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/lib/python3.11/site-packages (from langchain) (0.1.57)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain) (1.10.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.11/site-packages (from langchain) (2.28.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.11/site-packages (from langchain) (8.3.0)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading multidict-6.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading marshmallow-3.21.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain) (2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (0.4.3)\n",
      "Downloading langchain-0.1.20-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
      "Downloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (272 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m272.3/272.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m980.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.7/128.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (328 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m328.1/328.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: typing-inspect, multidict, marshmallow, frozenlist, yarl, dataclasses-json, aiosignal, aiohttp, langchain-text-splitters, langchain-community, langchain\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 dataclasses-json-0.6.6 frozenlist-1.4.1 langchain-0.1.20 langchain-community-0.0.38 langchain-text-splitters-0.0.1 marshmallow-3.21.2 multidict-6.0.5 typing-inspect-0.9.0 yarl-1.9.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/conda/lib/python3.11/site-packages (1.30.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from openai) (4.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from openai) (1.10.2)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.11/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.11/site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_openai\n",
      "  Downloading langchain_openai-0.1.6-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.46 (from langchain_openai)\n",
      "  Downloading langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting openai<2.0.0,>=1.24.0 (from langchain_openai)\n",
      "  Downloading openai-1.30.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting tiktoken<1,>=0.5.2 (from langchain_openai)\n",
      "  Downloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (1.33)\n",
      "Collecting langsmith<0.2.0,>=0.1.0 (from langchain-core<0.2.0,>=0.1.46->langchain_openai)\n",
      "  Downloading langsmith-0.1.57-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (1.10.2)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain-core<0.2.0,>=0.1.46->langchain_openai)\n",
      "  Downloading tenacity-8.3.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (4.0.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.24.0->langchain_openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.24.0->langchain_openai)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (4.8.0)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.5.2->langchain_openai)\n",
      "  Downloading regex-2024.5.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m963.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.11/site-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2.28.1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain_openai) (3.4)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.46->langchain_openai) (2.4)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.46->langchain_openai)\n",
      "  Downloading orjson-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain_openai) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain_openai) (1.26.12)\n",
      "Downloading langchain_openai-0.1.6-py3-none-any.whl (34 kB)\n",
      "Downloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.30.1-py3-none-any.whl (320 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langsmith-0.1.57-py3-none-any.whl (121 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.0/121.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.5.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m785.2/785.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
      "Downloading orjson-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tenacity, regex, orjson, httpcore, distro, tiktoken, langsmith, httpx, openai, langchain-core, langchain_openai\n",
      "Successfully installed distro-1.9.0 httpcore-1.0.5 httpx-0.27.0 langchain-core-0.1.52 langchain_openai-0.1.6 langsmith-0.1.57 openai-1.30.1 orjson-3.10.3 regex-2024.5.10 tenacity-8.3.0 tiktoken-0.7.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "#í…œí”Œë¦¿ ìƒì„± - ì±„íŒ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import getpass\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "import pandas as pd\n",
    "from langchain_core.output_parsers import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    }
   ],
   "source": [
    "# í™˜ê²½ë³€ìˆ˜ì— OpenAI API í‚¤ ì €ì¥ (ì‚¬ìš©ì ì…ë ¥ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ)\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "# GPT-4 ëª¨ë¸ë¡œ ì±—ë´‡ ì´ˆê¸°í™”\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3ê°€ì§€ í”„ë¡¬í”„íŠ¸ë¡œ êµ¬ë¶„\n",
    "1. ê¸€êµ¬ì¡° í”„ë¡¬í”„íŠ¸\n",
    "2. ê¸€ ìƒì„± í”„ë¡¬í”„íŠ¸\n",
    "3. ê¸€ í‰ê°€ í”„ë¡¬í”„íŠ¸\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.ê¸€ êµ¬ì¡° í”„ë¡¬í”„íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_benchmark=\"\"\"ì•ˆë…•í•˜ì„¸ìš”, ì—¬ëŸ¬ë¶„ì˜ ì¶œí‡´ê·¼ ë©”ì‹ ì € ì§€í•˜ì²  ì˜¨ë‹¤ì˜ 'ì˜¤.ì§€.í†µ [ì˜¤ëŠ˜ì˜ ì§€í•˜ì²  ì†Œì‹í†µ]' ì¸ì‚¬ ë“œë¦½ë‹ˆë‹¤!\n",
    "ì§€í•˜ì²  ì´ìš©ê° ì—¬ëŸ¬ë¶„, ì„œìš¸ ì§€í•˜ì²  ê²½ê³ íŒŒì—…ì— ëŒ€í•œ ì¤‘ìš”í•œ ì—…ë°ì´íŠ¸ê°€ ìˆìŠµë‹ˆë‹¤.\n",
    "â€‹\n",
    "íŒŒì—… ì¼ì •\n",
    "\n",
    "íŒŒì—… ì¼ì‹œ: 2023ë…„ 11ì›” 9ì¼ë¶€í„° 11ì›” 10ì¼ê¹Œì§€\n",
    "\n",
    "íŒŒì—… ë…¸ì„ : ì„œìš¸ì§€í•˜ì²  1âˆ¼8í˜¸ì„ , 9í˜¸ì„  2Â·3ë‹¨ê³„ êµ¬ê°„(ì‹ ë…¼í˜„âˆ¼ì¤‘ì•™ë³´í›ˆë³‘ì›ì—­)\n",
    "\n",
    "íŒŒì—… ì´ìœ : ì„ê¸ˆ ë° ë‹¨ì²´í˜‘ì•½ í˜‘ìƒ ê²°ë ¬, ì¸ë ¥ ê°ì¶• ë° ì•ˆì „ ì—…ë¬´ ì™¸ì£¼í™” ì² íšŒ ìš”êµ¬\n",
    "\n",
    "ë¬¸ì˜ ì‚¬í•­: ì„œìš¸êµí†µê³µì‚¬ í™ˆí˜ì´ì§€\n",
    "\n",
    "â€‹\n",
    "\n",
    "\n",
    "ì„œìš¸ ì§€í•˜ì² ì—ì„œ ì—´ë¦¬ëŠ” ê²½ê³ íŒŒì—… ì†Œì‹, ì—¬ëŸ¬ë¶„ë„ ë“¤ìœ¼ì…¨ë‚˜ìš”?\n",
    "\n",
    "ì„œìš¸ì˜ ì§€í•˜ì²  ë…¸ì„ ë“¤ì´ 11ì›” 9ì¼ë¶€í„° 10ì¼ê¹Œì§€ íŒŒì—…ì„ ì˜ˆì •í•˜ê³  ìˆì–´ìš”.\n",
    "\n",
    "ì•„ì¹¨ì €ë… ì¶œí‡´ê·¼ê¸¸ì´ ì¡°ê¸ˆ í˜ë“¤ì–´ì§ˆ ìˆ˜ ìˆìœ¼ë‹ˆ, ì €ì™€ í•¨ê»˜ ëŒ€ë¹„ì±…ì„ ì„¸ì›Œë´ìš”!\n",
    "\n",
    "íŒŒì—… ê¸°ê°„ ì¤‘ ëŒ€ì‘ ë°©ë²•ê³¼ ì¶œê·¼ê¸¸ ì¤€ë¹„ ì „ëµ\n",
    "\n",
    "ì‹œê°„ëŒ€ë³„ ì§€í•˜ì²  ì •ë³´ ì±™ê¸°ê¸°\n",
    "\n",
    "íŒŒì—… ë™ì•ˆì€ ì§€í•˜ì²  ìš´í–‰ì´ ì¡°ê¸ˆì”© ë‹¬ë¼ì§ˆ ìˆ˜ ìˆì–´ìš”.\n",
    "\n",
    "ì§‘ì„ ë‚˜ì„œê¸° ì „ì— ë¯¸ë¦¬ ì²´í¬í•˜ê³  ì‹œê°„ì„ ì¡°ì •í•´ë³´ì„¸ìš”!\n",
    "\n",
    "â€‹\n",
    "\n",
    "ëŒ€ì²´ êµí†µìˆ˜ë‹¨ ì ê·¹ í™œìš©í•˜ê¸°\n",
    "\n",
    "ë²„ìŠ¤ê°€ ì•„ì¹¨ 7ì‹œë¶€í„° 9ì‹œ, ì €ë… 6ì‹œë¶€í„° 8ì‹œê¹Œì§€ ì§‘ì¤‘ ë°°ì°¨ëœë‹¤ê³  í•˜ë‹ˆ,\n",
    "\n",
    "ë²„ìŠ¤ë¡œ ê°ˆì•„íƒ€ëŠ” ê²ƒë„ ì¢‹ì€ ì„ íƒì´ ë  ìˆ˜ ìˆì–´ìš”.\n",
    "\n",
    "â€‹\n",
    "\n",
    "ë¹„ìƒ ì—°ë½ë§ í™œìš©í•˜ê¸°\n",
    "\n",
    "íŒŒì—… ê¸°ê°„ ì¤‘ì—ëŠ” ìœ ì—°í•˜ê²Œ ê·¼ë¬´í•˜ê±°ë‚˜ ìˆ˜ì—…ì— ì°¸ì—¬í•  ìˆ˜ ìˆë„ë¡ íšŒì‚¬ë‚˜ í•™êµì™€ ì—°ë½í•´ ì¡°ìœ¨í•´ ë³´ì„¸ìš”.\n",
    "\n",
    "â€‹\n",
    "\n",
    "ë¹„ë¡ ì§€í•˜ì² ì´ ì ì‹œ ë©ˆì¶° ì„œë„ ìš°ë¦¬ì˜ ì¶œí‡´ê·¼ì€ ë©ˆì¶œ ìˆ˜ ì—†ì–ì•„ìš”!\n",
    "\n",
    "ì„œìš¸êµí†µê³µì‚¬ì™€ ì„œìš¸ì‹œê°€ ë¶ˆí¸ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ë…¸ë ¥ ì¤‘ì´ë‹ˆ,\n",
    "\n",
    "ì¡°ê¸ˆë§Œ ì°¸ê³  ìš°ë¦¬ ëª¨ë‘ í•¨ê»˜ ì´ ì–´ë ¤ìš´ ì‹œê¸°ë¥¼ ì˜ ë„˜ê²¨ë´ìš”.\n",
    "\n",
    "â€‹\n",
    "\n",
    "ì˜¤ì§€í†µê³¼ í•¨ê»˜ë¼ë©´ ì¶œí‡´ê·¼ë„ ì¦ê±°ì›Œì§ˆ ìˆ˜ ìˆì–´ìš”, ìì£¼ ë“¤ëŸ¬ì„œ ìµœì‹  ì†Œì‹ë„ í™•ì¸í•˜ê³  í˜ë‚´ì„¸ìš”!\n",
    "\n",
    "â€‹\n",
    "\n",
    "â€‹\n",
    "\n",
    "â€‹\n",
    "\n",
    "â€‹\n",
    "\n",
    "'ì§€í•˜ì²  ì˜¨ë‹¤'ëŠ” ë‹¨ í•œ ë²ˆì˜ í„°ì¹˜ë¡œ\n",
    "\n",
    "ìì‹ ì˜ ìœ„ì¹˜ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ì§€í•˜ì² ì—­ì˜\n",
    "\n",
    "ì‹¤ì‹œê°„ ë„ì°© ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤\n",
    "\n",
    "â€‹\n",
    "\n",
    "ğŸ”½ ì§€í•˜ì²  ì˜¨ë‹¤ ì†Œê°œ ë³´ëŸ¬ê°€ê¸°\n",
    "\n",
    "https://blog.naver.com/subway__onda/223258646349\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Extract the table of structure from the provided text in English. Please don't give me the details of the structure of the article JUST STRUCTURE 1: Greeting 2:\"), #\"ëª©ì°¨ì •ë³´ ì¶”ì¶œ\"\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "chain= prompt1|llm\n",
    "Structure=chain.invoke({\"input\":page_benchmark})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1: Greeting\\n2: Strike Update Announcement\\n3: Strike Details\\n4: Additional Explanatory Information\\n5: Strike Preparedness Suggestions\\n6: Encouragement and Conclusion\\n7: App Advertisement'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Structure.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ê¸€ êµ¬ì¡° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í˜• (task ìª¼ê°œê¸° ìœ„í•¨)\n",
    "import re\n",
    "#ê¸€ì˜ êµ¬ì¡°ì—ì„œ ë„ˆë¬´ ìì„¸í•œ ì •ë³´ëŠ” ì§ì ‘ ì œê±°í•˜ì˜€ìŒ.\n",
    "output1 = '1: Greeting\\n2: Strike Update Announcement\\n3: Strike Details\\n4: Additional Explanatory Information\\n5: Strike Preparedness Suggestions\\n6: Encouragement and Conclusion\\n7: App Advertisement'\n",
    "# Using regex to split the string into sections starting with number and period\n",
    "sections = re.split(r'\\n(?=\\d+\\.)', output1)\n",
    "section=\"\"\n",
    "for i in (sections):\n",
    "          section+=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1: Greeting\\n2: Strike Update Announcement\\n3: Strike Details\\n4: Additional Explanatory Information\\n5: Strike Preparedness Suggestions\\n6: Encouragement and Conclusion\\n7: App Advertisement'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ê¸€ ìƒì„± í”„ë¡¬í”„íŠ¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### source ê¸°ì‚¬ (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_link=\"https://www.sedaily.com/NewsView/29X8F079IZ\"\n",
    "source_content=\"\"\"\"ë˜ í‡´ê·¼ê¸¸ ëŒ€ë€ ì¼ì–´ë‚˜ë‚˜\"â€¦22ì¼ ì„œìš¸ì§€í•˜ì²  2ì°¨ ì´íŒŒì—… ì˜ˆê³ \n",
    "ì¸ë ¥ê°ì¶• ë†“ê³  ë…¸ì‚¬ í•©ì˜ì  ëª»ì°¾ì•„\n",
    "\n",
    "í™©ì •ì› ê¸°ì2023-11-14 17:57:36ì‚¬íšŒì¼ë°˜\n",
    "'ë˜ í‡´ê·¼ê¸¸ ëŒ€ë€ ì¼ì–´ë‚˜ë‚˜'â€¦22ì¼ ì„œìš¸ì§€í•˜ì²  2ì°¨ ì´íŒŒì—… ì˜ˆê³ \n",
    "ì„œìš¸êµí†µê³µì‚¬ ë…¸ì¡° íŒŒì—… ì´í‹€ì§¸ì¸ 10ì¼ ì˜¤ì „ ì„œìš¸ ì§€í•˜ì²  ê´‘í™”ë¬¸ì—­ì—ì„œ ì‹œë¯¼ë“¤ì´ ì—´ì°¨ì—ì„œ ë‚´ë¦¬ê³  ìˆë‹¤. ì—°í•©ë‰´\n",
    "\n",
    "\n",
    "ì„œìš¸ì§€í•˜ì²  1âˆ¼8í˜¸ì„ ì„ ìš´ì˜í•˜ëŠ” ì„œìš¸êµí†µê³µì‚¬ ë…¸ë™ì¡°í•©ì´ 22ì¼ 2ì°¨ ì´íŒŒì—…ì„ ì˜ˆê³ í–ˆë‹¤.\n",
    "\n",
    "\n",
    "ë¯¼ì£¼ë…¸ì´ ê³µê³µìš´ìˆ˜ë…¸ì¡° ì‚°í•˜ ì„œìš¸êµí†µê³µì‚¬ë…¸ì¡°ëŠ” 15ì¼ ì˜¤ì „ 10ì‹œ ì„œìš¸ì‹œì²­ ì•ì—ì„œ â€œì•ˆì „ì—…ë¬´ ì™¸ì£¼í™” ì €ì§€, í˜„ì¥ ì•ˆì „ì¸ë ¥ ê³µë°± ëŒ€ì±… ìˆ˜ë¦½â€ì„ ìš”êµ¬í•˜ëŠ” ê¸°ìíšŒê²¬ì„ ì—´ê³  2ì°¨ íŒŒì—… ê³„íšì„ ê³µê°œí•  ì˜ˆì •ì´ë¼ê³  14ì¼ ë°í˜”ë‹¤. íŒŒì—… ì˜ˆì •ì¼ì€ 22ì¼ë¡œ ì•Œë ¤ì¡Œë‹¤.\n",
    "\n",
    "\n",
    "ë…¸ì¡°ëŠ” ì‚¬ì¸¡ì˜ ëŒ€ê·œëª¨ ì¸ë ¥ê°ì¶•ì•ˆ ë“±ì— ë°˜ë°œí•´ ì§€ë‚œ 9~10ì¼ ì´í‹€ê°„ ì‹œí•œë¶€ ê²½ê³  íŒŒì—…ì„ ë²Œì˜€ë‹¤. ì´í›„ ë…¸ì‚¬ ì‹¤ë¬´ì ê°„ ëŒ€í™”ë¥¼ ì´ì–´ê°€ê³ ëŠ” ìˆìœ¼ë‚˜ ì•„ì§ í•©ì˜ì ì„ ì°¾ì§€ëŠ” ëª»í–ˆë‹¤.\n",
    "\n",
    "\n",
    "ë…¸ì¡°ëŠ” \"1ì°¨ ê²½ê³  íŒŒì—… ì´í›„ ì§€ì†ì ìœ¼ë¡œ ì„œìš¸ì‹œì™€ ê³µì‚¬ ì¸¡ì— ì…ì¥ ë³€í™”ì™€ ì§„ì§€í•œ ëŒ€í™”ë¥¼ ì´‰êµ¬í–ˆì§€ë§Œ, ì„œìš¸ì‹œì™€ ê³µì‚¬ëŠ” ê°‘ìê¸° ê°•ë ¥ ëŒ€ì‘ ê¸°ì¡°ë¡œ ëŒë³€í–ˆë‹¤\"ë©° \"í˜„ì¥ ì•ˆì „ì¸ë ¥ ê³µë°± ìš°ë ¤ì— ëŒ€í•œ ì–´ë–¤ ëŒ€ì±…ë„ ì œì‹œí•˜ì§€ ì•Šê³  ìˆë‹¤\"ê³  ë°í˜”ë‹¤. ë…¸ì¡°ëŠ” ê¸°ìíšŒê²¬ì—ì„œ ìì„¸í•œ íŒŒì—… ê³„íšê³¼ í–¥í›„ ì¼ì •ì„ ë°í ì˜ˆì •ì´ë‹¤.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê¸€ìƒì„± ì‹œì‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: The system utilizes the structure from existing blog benchmark articles and creates new blog posts using information from new source data.\n",
      "          structure from existing blog benchmark articles : 1: Greeting\n",
      "2: Strike Update Announcement\n",
      "3: Strike Details\n",
      "4: Additional Explanatory Information\n",
      "5: Strike Preparedness Suggestions\n",
      "6: Encouragement and Conclusion\n",
      "7: App Advertisement\n",
      "          Once you obtain information from the source data, you will write the article considering the structure from existing blog benchmark articles.\n",
      "          \n",
      "Human: source data:{\"ë˜ í‡´ê·¼ê¸¸ ëŒ€ë€ ì¼ì–´ë‚˜ë‚˜\"â€¦22ì¼ ì„œìš¸ì§€í•˜ì²  2ì°¨ ì´íŒŒì—… ì˜ˆê³ \n",
      "ì¸ë ¥ê°ì¶• ë†“ê³  ë…¸ì‚¬ í•©ì˜ì  ëª»ì°¾ì•„\n",
      "\n",
      "í™©ì •ì› ê¸°ì2023-11-14 17:57:36ì‚¬íšŒì¼ë°˜\n",
      "'ë˜ í‡´ê·¼ê¸¸ ëŒ€ë€ ì¼ì–´ë‚˜ë‚˜'â€¦22ì¼ ì„œìš¸ì§€í•˜ì²  2ì°¨ ì´íŒŒì—… ì˜ˆê³ \n",
      "ì„œìš¸êµí†µê³µì‚¬ ë…¸ì¡° íŒŒì—… ì´í‹€ì§¸ì¸ 10ì¼ ì˜¤ì „ ì„œìš¸ ì§€í•˜ì²  ê´‘í™”ë¬¸ì—­ì—ì„œ ì‹œë¯¼ë“¤ì´ ì—´ì°¨ì—ì„œ ë‚´ë¦¬ê³  ìˆë‹¤. ì—°í•©ë‰´\n",
      "\n",
      "\n",
      "ì„œìš¸ì§€í•˜ì²  1âˆ¼8í˜¸ì„ ì„ ìš´ì˜í•˜ëŠ” ì„œìš¸êµí†µê³µì‚¬ ë…¸ë™ì¡°í•©ì´ 22ì¼ 2ì°¨ ì´íŒŒì—…ì„ ì˜ˆê³ í–ˆë‹¤.\n",
      "\n",
      "\n",
      "ë¯¼ì£¼ë…¸ì´ ê³µê³µìš´ìˆ˜ë…¸ì¡° ì‚°í•˜ ì„œìš¸êµí†µê³µì‚¬ë…¸ì¡°ëŠ” 15ì¼ ì˜¤ì „ 10ì‹œ ì„œìš¸ì‹œì²­ ì•ì—ì„œ â€œì•ˆì „ì—…ë¬´ ì™¸ì£¼í™” ì €ì§€, í˜„ì¥ ì•ˆì „ì¸ë ¥ ê³µë°± ëŒ€ì±… ìˆ˜ë¦½â€ì„ ìš”êµ¬í•˜ëŠ” ê¸°ìíšŒê²¬ì„ ì—´ê³  2ì°¨ íŒŒì—… ê³„íšì„ ê³µê°œí•  ì˜ˆì •ì´ë¼ê³  14ì¼ ë°í˜”ë‹¤. íŒŒì—… ì˜ˆì •ì¼ì€ 22ì¼ë¡œ ì•Œë ¤ì¡Œë‹¤.\n",
      "\n",
      "\n",
      "ë…¸ì¡°ëŠ” ì‚¬ì¸¡ì˜ ëŒ€ê·œëª¨ ì¸ë ¥ê°ì¶•ì•ˆ ë“±ì— ë°˜ë°œí•´ ì§€ë‚œ 9~10ì¼ ì´í‹€ê°„ ì‹œí•œë¶€ ê²½ê³  íŒŒì—…ì„ ë²Œì˜€ë‹¤. ì´í›„ ë…¸ì‚¬ ì‹¤ë¬´ì ê°„ ëŒ€í™”ë¥¼ ì´ì–´ê°€ê³ ëŠ” ìˆìœ¼ë‚˜ ì•„ì§ í•©ì˜ì ì„ ì°¾ì§€ëŠ” ëª»í–ˆë‹¤.\n",
      "\n",
      "\n",
      "ë…¸ì¡°ëŠ” \"1ì°¨ ê²½ê³  íŒŒì—… ì´í›„ ì§€ì†ì ìœ¼ë¡œ ì„œìš¸ì‹œì™€ ê³µì‚¬ ì¸¡ì— ì…ì¥ ë³€í™”ì™€ ì§„ì§€í•œ ëŒ€í™”ë¥¼ ì´‰êµ¬í–ˆì§€ë§Œ, ì„œìš¸ì‹œì™€ ê³µì‚¬ëŠ” ê°‘ìê¸° ê°•ë ¥ ëŒ€ì‘ ê¸°ì¡°ë¡œ ëŒë³€í–ˆë‹¤\"ë©° \"í˜„ì¥ ì•ˆì „ì¸ë ¥ ê³µë°± ìš°ë ¤ì— ëŒ€í•œ ì–´ë–¤ ëŒ€ì±…ë„ ì œì‹œí•˜ì§€ ì•Šê³  ìˆë‹¤\"ê³  ë°í˜”ë‹¤. ë…¸ì¡°ëŠ” ê¸°ìíšŒê²¬ì—ì„œ ìì„¸í•œ íŒŒì—… ê³„íšê³¼ í–¥í›„ ì¼ì •ì„ ë°í ì˜ˆì •ì´ë‹¤.\n",
      "}Do not start writing until you are given a command. Just Review source data\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: The system utilizes the structure from existing blog benchmark articles and creates new blog posts using information from new source data.\n",
      "          structure from existing blog benchmark articles : 1: Greeting\n",
      "2: Strike Update Announcement\n",
      "3: Strike Details\n",
      "4: Additional Explanatory Information\n",
      "5: Strike Preparedness Suggestions\n",
      "6: Encouragement and Conclusion\n",
      "7: App Advertisement\n",
      "          Once you obtain information from the source data, you will write the article considering the structure from existing blog benchmark articles.\n",
      "          \n",
      "System: The Seoul Subway labor union has announced a second general strike for November 22, protesting against staff reductions and outsourcing of safety tasks, with no resolution reached between the union and management despite previous strikes and ongoing discussions.\n",
      "Human: 1: Greeting\n",
      "2: Strike Update Announcement\n",
      "3: Strike Details\n",
      "4: Additional Explanatory Information\n",
      "5: Strike Preparedness Suggestions\n",
      "6: Encouragement and Conclusion\n",
      "7: App Advertisement\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: The system utilizes the structure from existing blog benchmark articles and creates new blog posts using information from new source data.\n",
      "          structure from existing blog benchmark articles : 1: Greeting\n",
      "2: Strike Update Announcement\n",
      "3: Strike Details\n",
      "4: Additional Explanatory Information\n",
      "5: Strike Preparedness Suggestions\n",
      "6: Encouragement and Conclusion\n",
      "7: App Advertisement\n",
      "          Once you obtain information from the source data, you will write the article considering the structure from existing blog benchmark articles.\n",
      "          \n",
      "System: The Seoul Subway labor union has announced a second general strike for November 22, protesting against staff reductions and outsourcing of safety tasks, with no resolution reached between the union and management despite previous strikes and ongoing discussions. The union emphasizes that these changes could compromise safety and efficiency, urging the need for adequate staffing and direct management of safety measures. Commuters are advised to seek alternative transportation options and stay updated through local news and official channels. The 'City Transit Helper' app is recommended for real-time updates and travel planning during the strike.\n",
      "Human: Translate English to Korean\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: The system utilizes the structure from existing blog benchmark articles and creates new blog posts using information from new source data.\n",
      "          structure from existing blog benchmark articles : 1: Greeting\n",
      "2: Strike Update Announcement\n",
      "3: Strike Details\n",
      "4: Additional Explanatory Information\n",
      "5: Strike Preparedness Suggestions\n",
      "6: Encouragement and Conclusion\n",
      "7: App Advertisement\n",
      "          Once you obtain information from the source data, you will write the article considering the structure from existing blog benchmark articles.\n",
      "          \n",
      "System: The Seoul Subway labor union has announced a second general strike for November 22, protesting against staff reductions and outsourcing of safety tasks, with no resolution reached between the union and management despite previous strikes and ongoing discussions. The union emphasizes that these changes could compromise safety and efficiency, urging the need for adequate staffing and direct management of safety measures. Commuters are advised to seek alternative transportation options and stay updated through local news and official channels. The 'City Transit Helper' app is recommended for real-time updates and travel planning during the strike. The announcement and details of the strike have also been translated into Korean for broader accessibility.\n",
      "Human: ëŒ€í•™ìƒì„œí¬í„°ì¦ˆê°€ ì‘ì„±í•œ ê²ƒ ì²˜ëŸ¼ í†µí†µ íŠ€ê²Œ ì‘ì„±í•´ì¤˜.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate(\n",
    "messages=[\n",
    "SystemMessagePromptTemplate.from_template(\n",
    "          \"\"\"The system utilizes the structure from existing blog benchmark articles and creates new blog posts using information from new source data.\n",
    "          structure from existing blog benchmark articles : \"\"\"+section+\"\"\"\n",
    "          Once you obtain information from the source data, you will write the article considering the structure from existing blog benchmark articles.\n",
    "          \"\"\"\n",
    "),\n",
    "# The `variable_name` here is what must align with memory\n",
    "MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "]\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100,memory_key=\"chat_history\", return_messages=True)\n",
    "conversation = LLMChain(\n",
    "llm=llm,\n",
    "prompt=prompt,\n",
    "verbose=True,\n",
    "memory=memory\n",
    ")\n",
    "\n",
    "conversation({\"question\":'source data:{'+source_content +'}Do not start writing until you are given a command. Just Review source data'})\n",
    "content=\"\"\n",
    "for j in sections:\n",
    "          content+=conversation.predict(question=j)\n",
    "#ì‹œìŠ¤í…œí”„ë¡¬í”„íŠ¸ìˆ˜ì • - ë²ˆì—­ & ëŒ€í•™ìƒì„œí¬í„°ì¦ˆ\n",
    "conversation.predict(question=\"Translate English to Korean\")\n",
    "result=conversation.predict(question=\"ëŒ€í•™ìƒì„œí¬í„°ì¦ˆê°€ ì‘ì„±í•œ ê²ƒ ì²˜ëŸ¼ í†µí†µ íŠ€ê²Œ ì‘ì„±í•´ì¤˜.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ì•ˆë…•í•˜ì„¸ìš”, ì—¬ëŸ¬ë¶„! ì„œìš¸ ì§€í•˜ì² ì—ì„œ ë˜ ë‹¤ì‹œ ì†Œì‹ì´ ì „í•´ì¡ŒìŠµë‹ˆë‹¤! ì´ë²ˆì—ëŠ” ë…¸ì¡°ê°€ 11ì›” 22ì¼ì— ë‘ ë²ˆì§¸ íŒŒì—…ì„ ì˜ˆê³ í–ˆì–´ìš”. ì´ìœ ë¥¼ ë“¤ì–´ë³´ë‹ˆ, ì§ì› ê°ì¶•ê³¼ ì•ˆì „ ì—…ë¬´ì˜ ì™¸ì£¼í™”ì— ëŒ€í•œ ë°˜ëŒ€ë¼ê³  í•˜ë„¤ìš”. ì´ì „ íŒŒì—…ê³¼ ë…¼ì˜ì—ë„ ë¶ˆêµ¬í•˜ê³  ì•„ì§ í•´ê²°ì±…ì„ ì°¾ì§€ ëª»í–ˆë‹¤ê³  í•´ìš”.\\n\\në…¸ì¡° ì¸¡ì€ ì´ëŸ° ë³€í™”ê°€ ì•ˆì „ì„±ê³¼ íš¨ìœ¨ì„±ì„ ìœ„í˜‘í•  ìˆ˜ ìˆë‹¤ë©°, ì ì ˆí•œ ì¸ë ¥ ë°°ì¹˜ì™€ ì•ˆì „ ê´€ë¦¬ì˜ ì§ì ‘ ìˆ˜í–‰ì„ ê°•ì¡°í•˜ê³  ìˆì–´ìš”. ì´ì— ë”°ë¼, íŒŒì—… ë‚ ì§œì—ëŠ” ì„œìš¸ ì§€í•˜ì²  ì´ìš©ì— ì°¨ì§ˆì´ ì˜ˆìƒë˜ë‹ˆ, ëŒ€ì¤‘êµí†µ ì´ìš©ì ì—¬ëŸ¬ë¶„ë“¤ì€ ë¯¸ë¦¬ ë‹¤ë¥¸ êµí†µ ìˆ˜ë‹¨ì„ ì°¾ì•„ë³´ì‹œëŠ” ê²Œ ì¢‹ê² ì–´ìš”.\\n\\nê·¸ë¦¬ê³ , íŒŒì—… ë„ì¤‘ì—ëŠ” ì‹¤ì‹œê°„ êµí†µ ì—…ë°ì´íŠ¸ì™€ ì—¬í–‰ ê³„íšì„ ë„ì™€ì¤„ ìˆ˜ ìˆëŠ” 'City Transit Helper' ì•± ì‚¬ìš©ì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤. ì´ ì•±ì€ íŒŒì—… ìƒí™©ì—ì„œë„ ì—¬ëŸ¬ë¶„ì´ ì›í™œí•˜ê²Œ ëª©ì ì§€ê¹Œì§€ ì´ë™í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤„ ê±°ì˜ˆìš”.\\n\\në…¸ì¡°ì˜ ë°œí‘œì™€ íŒŒì—… ì„¸ë¶€ì‚¬í•­ì€ í•œêµ­ì–´ë¡œë„ ë²ˆì—­ë˜ì–´ ë” ë§ì€ ì‚¬ëŒë“¤ì´ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ í–ˆìœ¼ë‹ˆ, ê´€ë ¨ ì†Œì‹ì€ ê¼­ í™•ì¸í•´ë³´ì„¸ìš”.\\n\\nì—¬ëŸ¬ë¶„, ì´ë²ˆ íŒŒì—… ê¸°ê°„ ë™ì•ˆ ë¶ˆí¸í•¨ì„ ìµœì†Œí™”í•  ìˆ˜ ìˆë„ë¡ ë¯¸ë¦¬ ì¤€ë¹„í•˜ì‹œê³ , ì–¸ì œë‚˜ ì•ˆì „ ìš´ì „í•˜ì„¸ìš”! íŒŒì—…ì´ ë¹¨ë¦¬ í•´ê²°ë˜ì–´ ì¼ìƒì´ ì›í™œí•˜ê²Œ ëŒì•„ê°€ê¸¸ ë°”ëë‹ˆë‹¤. ì—¬ëŸ¬ë¶„ì˜ ëˆê¸°ì™€ í˜‘ë ¥ì´ í•„ìš”í•œ ì‹œê¸°ì…ë‹ˆë‹¤. í•¨ê»˜ í˜ë‚´ìš”! ğŸ˜Š\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.ê¸€ í‰ê°€ í”„ë¡¬í”„íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "consistency=[]\n",
    "coherence=[]\n",
    "friendliness=[]\n",
    "fluency=[]\n",
    "humanlikeness=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1 í‰ê°€ ë°©ë²• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval=[\"\"\"Compare and evaluate the blog content you created with the article.\n",
    "First, Extract the key information from the article. (time,date,subway line)\n",
    "You need to decide whether the key information is entailed by the  CONTEXT by choosing one of the following rating: \n",
    "\n",
    "1. 5 point: The blog content follows logically from the information contained in the article. \n",
    "\n",
    "2. 1 point: The blog content is logically false from the information contained in the article. \n",
    "\n",
    "3. an integer score between 1 and 5 and if such integer score does not exist,  \n",
    "\n",
    "use 1: It is not possible to determine whether the blog content is true or false without further information. \n",
    "\n",
    "Read the passage of information thoroughly and select the correct answer from the three answer labels. \n",
    "\n",
    "Read the CONTEXT thoroughly to ensure you know what the CONTEXT entails.  \n",
    "\n",
    "Note the blog content is generated by a computer system, it can contain certain symbols, which should not be a negative factor in the evaluation.\n",
    "\n",
    "\n",
    "Evaluation Form (scores ONLY):\n",
    "\n",
    "- Consistency:\n",
    "          \n",
    "          \"\"\",\n",
    "\"\"\"Your task is to rate the blog content on one metric.\n",
    "\n",
    "          Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "          Evaluation Criteria:\n",
    "\n",
    "          Coherence (1-5) - the collective quality of all sentences. We align this dimension with the DUC quality question of structure and coherence whereby \"the summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to a coherent body of information about a topic.\"\n",
    "\n",
    "          Evaluation Steps:\n",
    "\n",
    "          1. Read the news article carefully and identify the main topic and key points.\n",
    "          2. Read the blog content and compare it to the news article. Check if the summary covers the main topic and key points of the news article, and if it presents them in a clear and logical order.\n",
    "          3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.\n",
    "\n",
    "\n",
    "\n",
    "          Evaluation Form (scores ONLY):\n",
    "\n",
    "          - Coherence:\n",
    "          \n",
    "          \"\"\",\n",
    "\"\"\"Friendliness, as a metric, measures how effectively the text of a blog content engages the reader with a friendly and inviting tone. If the tone and style of the text foster a sense of warmth and familiarity, making the reader feel welcomed and engaged, then the value of the Friendliness metric should be high; otherwise, it should be low. After reading the blog post, evaluate its tone and style against typical friendly communication, and determine the value of the Friendliness metric using the following rating scale:\n",
    "\n",
    "          - One star: The tone is cold or unfriendly, not engaging the reader in any appreciable way.\n",
    "          - Two stars: The tone has minimal warmth, slightly engaging but largely formal or distant.\n",
    "          - Three stars: The tone is moderately friendly, balancing formal and informal elements to somewhat engage the reader.\n",
    "          - Four stars: The tone is very friendly, actively engaging the reader with warmth and lively expressions.\n",
    "          - Five stars: The tone is exceptionally friendly and lively, creating a strong sense of connection and engagement with the reader.\n",
    "\n",
    "          This rating should always be an integer between 1 and 5. Thus, the rating produced should be 1, 2, 3, 4, or 5, based on your assessment of how friendly and engaging the text is towards the read.\n",
    "\n",
    "          Evaluation Template (Score Only):\n",
    "\n",
    "          Friendliness:\n",
    "          \"\"\",\n",
    "\"\"\"Fluency measures the quality of individual sentences in the answer, and whether they are well-written and grammatically correct. Consider the quality of individual sentences when evaluating fluency. Given the question and answer, score the fluency of the answer between one to five stars using the following rating scale: \n",
    "\n",
    "          One star: the answer completely lacks fluency \n",
    "\n",
    "          Two stars: the answer mostly lacks fluency \n",
    "\n",
    "          Three stars: the answer is partially fluent \n",
    "\n",
    "          Four stars: the answer is mostly fluent \n",
    "\n",
    "          Five stars: the answer has perfect fluency \n",
    "\n",
    "          This rating value should always be an integer between 1 and 5. So the rating produced should be 1 or 2 or 3 or 4 or 5.\n",
    "\n",
    "\n",
    "          Evaluation Template (Score Only):\n",
    "\n",
    "          Fluency:\n",
    "          \"\"\",\n",
    "\"\"\"Human-likeness, as a metric, evaluates how closely the style and delivery of GPT-generated text resemble that of naturally written human text. If the text is indistinguishable from human writing in terms of natural flow, vocabulary use, and contextual accuracy, then the value of the Human-likeness metric should be high; otherwise, it should be low. Given the text generated by GPT and comparing it to typical human-written text, determine the value of the Human-likeness metric using the following rating scale:\n",
    "\n",
    "          - One star: The text is entirely machine-like, with unnatural flow and numerous contextual errors.\n",
    "          - Two stars: The text is mostly machine-like, with frequent unnatural expressions and noticeable errors.\n",
    "          - Three stars: The text is somewhat human-like, showing signs of natural language use but with some mechanical characteristics.\n",
    "          - Four stars: The text is mostly human-like, with very few mechanical traits and generally natural language use.\n",
    "          - Five stars: The text is completely human-like, indistinguishable from something a human might write.\n",
    "\n",
    "          This rating should always be an integer between 1 and 5. Thus, the rating produced should be 1, 2, 3, 4, or 5, based on the evaluation.\n",
    "          \"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. í‰ê°€ ì‹œì‘ (í‰ê°€ì—ì„œëŠ” ê°ê°ì˜ ìƒˆë¡œìš´ ì±„íŒ…ìœ¼ë¡œ í•˜ì˜€ìŒ) -> memory ì‚¬ìš©x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ì •í™•ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()\n",
    "#ì •í™•ì„±- ê¸°ì‚¬(source)ì™€ ë¸”ë¡œê·¸ ì½˜í…ì¸  ëª¨ë‘ ë„£ì–´ì•¼í•¨\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", eval[0]),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "chain = prompt | llm | output_parser\n",
    "consistency+=[chain.invoke({\"input\": \"blog content:\"+result+\"article, source:\"+source_content})]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", eval[1]),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "chain=prompt|llm|output_parser\n",
    "coherence+=[chain.invoke({\"input\": \"blog content:\"+result})]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Friendliness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", eval[2]),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "chain=prompt|llm|output_parser\n",
    "friendliness+=[chain.invoke({\"input\": \"blog content:\"+result})]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fluency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", eval[3]),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "chain=prompt|llm|output_parser\n",
    "fluency+=[chain.invoke({\"input\": \"blog content:\"+result})]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Human-likeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", eval[4]),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "chain=prompt|llm|output_parser\n",
    "humanlikeness+=[chain.invoke({\"input\": \"blog content:\"+result})]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df,pd.DataFrame({\"link\":[source_link],\"source\":[source_content],\"result\":[result],\"consistency\":[consistency],\"coherence\":[coherence],\"friendliness\":[friendliness],\"human_likeness\":[humanlikeness]})])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"fianl_result_5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
