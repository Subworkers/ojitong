{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **ğŸ¯ GPT ê¸°ë°˜ ì •ëŸ‰í‰ê°€ : : QG-QA**\n",
        "## **ğŸ“ë¬¸ì œ ì •ì˜**\n",
        "- ë‰´ìŠ¤ ê¸°ì‚¬ ê°ê°ì— ëŒ€í•´ LLMì„ ì ìš©í•˜ì—¬ ê¸°ì‚¬ì— ëŒ€í•œ ë¸”ë¡œê·¸ ê¸€ì„ ë§Œë“  ìƒí™©ì—ì„œ,\n",
        "- ë¸”ë¡œê·¸ ê¸€ì´ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ë‚´ìš©ì„ ì˜ ë‹´ê³  ìˆëŠ”ì§€ QG-QAë¥¼ í†µí•´ í‰ê°€í•˜ê³ ì í•œë‹¤.\n",
        "- ì‚¬ìš© ëª¨ë¸: **gpt-4o**\n",
        "\n",
        "## **ğŸ“ìˆ˜í–‰ ê³¼ì •**\n",
        "1. **ë‰´ìŠ¤ ê¸°ì‚¬ì™€ ê·¸ì— ëŒ€ì‘í•˜ëŠ” ë¸”ë¡œê·¸ ê¸€ì— ëŒ€í•´ ë™ì¼í•œ ì§ˆë¬¸ì„ ìƒì„±**\n",
        "  - OUTPUT ì´ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ê°€ ë˜ë„ë¡ PROMPT êµ¬ì„±í•˜ê¸°\n",
        "2. **ë‰´ìŠ¤ ê¸°ì‚¬, ë¸”ë¡œê·¸ ê¸€ì— ì§ˆë¬¸ ê°ê° ASK**\n",
        "3. **ê·¸ì— ëŒ€í•œ ë‹µë³€ì´ ì„œë¡œ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•œë‹¤.**\n",
        "  - ë‘ ë‹µë³€ì„ ì„œë¡œ ë¹„êµí•˜ì—¬ ì¼ì¹˜í•  ê²½ìš° True(1), ì•„ë‹ ê²½ìš° False(0) ë°˜í™˜\n",
        "\n",
        "## **ğŸ“ì„ íƒì§€**\n",
        "1. **í•œë²ˆì— í•œê°œì˜ ì§ˆë¬¸ë§Œ / í•œë²ˆì— ì—¬ëŸ¬ê°œì˜ ì§ˆë¬¸?** â†’ *ì—¬ëŸ¬ê°œì˜ ì§ˆë¬¸*\n",
        "2. **í˜ë¥´ì†Œë‚˜ 1ê°œë¡œ í†µì¼/ì§ˆë¬¸ë‹µë³€ í˜ë¥´ì†Œë‚˜ 2ê°œë¡œ ë¶„ë¦¬?** â†’ *2ê°œ*\n",
        "3. **ë©”ëª¨ë¦¬ ì‚¬ìš©/ë¯¸ì‚¬ìš©?** â†’ *ë¯¸ì‚¬ìš©*\n",
        "\n",
        "## **ğŸ“ê¸°íƒ€ì‚¬í•­**\n",
        "1. ì§ˆë¬¸ 2ê°œë¡œ ì¶•ì†Œ(ì‚¬ê±´ ë°œìƒ ì›ì¸ì€ ë‹¤ë¥¸ ì§ˆë¬¸ì— ë¹„í•´ ì¤‘ìš”ì„±ê³¼ ì •í™•ì„±ì´ ë‚®ì•„ ì œê±°í•¨)\n",
        "2. ì§ˆë¬¸ë§ˆë‹¤ 5ê°œì˜ ì„ ì§€, 'ì•Œ ìˆ˜ ì—†ìŒ; ì„ ì§€ëŠ” ë¬´ì¡°ê±´ í¬í•¨\n",
        "3. QGì™€ QAë‹¨ê³„ ëª¨ë‘ì— Step ì„ ì§€ì •í•¨\n",
        "4. gpt-4-turbo ëŒ€ì‹  gpt-4o ì‚¬ìš© - ë¹„ìš© ì ˆê°\n",
        "5. ***ğŸš¨í•´ë‹¹ í•¨ìˆ˜ì— ë‰´ìŠ¤ì™€ ë¸”ë¡œê·¸ ë°ì´í„°ë¥¼ ì£¼ì…í•  ë•Œ 'ë°˜ë“œì‹œ' ë‰´ìŠ¤ëŠ” ì œëª©, ë°œí–‰ì¼ì, ë³¸ë¬¸ì´ ë¶„ë¦¬ëœ ìƒíƒœì¼ ê²ƒ***\n",
        "\n",
        "## **ğŸ“ì§„í–‰ ìˆœì„œ**\n",
        "\n",
        "        * (1) ì…ë ¥ - ë‰´ìŠ¤ ì œëª©, ë‰´ìŠ¤ ë³¸ë¬¸, ë‰´ìŠ¤ ë‚ ì§œ, ë¸”ë¡œê·¸ í…ìŠ¤íŠ¸ â†’ news_sample, blog_sample\n",
        "        * (2) RPOMPT1 - ë‰´ìŠ¤ê¸°ì‚¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì„ ë§Œë“¤ì–´ì¤˜- ì§ˆë¬¸ ë„ì¶œ\n",
        "        * (3) PROMPT2 - ë‰´ìŠ¤ ê¸°ì‚¬ì— ëŒ€í•´ ì§ˆë¬¸ì˜ ë‹µë³€ ë„ì¶œ\n",
        "        * (4) PROMPT3 - ë¸”ë¡œê·¸ ê¸€ì— ëŒ€í•´ ì§ˆë¬¸ì˜ ë‹µë³€ ë„ì¶œ\n",
        "        * (5)ì¼ì¹˜ì—¬ë¶€íŒë‹¨ - (3), (4)ì˜ ê²°ê³¼ë¡œ ë‚˜ì˜¨ ë‹µë³€ì„ ë¹„êµ"
      ],
      "metadata": {
        "id": "OQzY3pSslLA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ğŸ“1. í™˜ê²½ì„¸íŒ…**"
      ],
      "metadata": {
        "id": "f9IJJxwrlaWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai\n",
        "!pip install langchain\n",
        "!pip install langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4KSo5ydlfKk",
        "outputId": "4341ab8f-eff4-4d43-bd54-3c163496f85f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.30.3-py3-none-any.whl (320 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.30.3\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.2.1-py3-none-any.whl (973 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_core-0.2.1-py3-none-any.whl (308 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m308.5/308.5 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.63-py3-none-any.whl (122 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.8/122.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Installing collected packages: packaging, orjson, jsonpointer, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-2.4 langchain-0.2.1 langchain-core-0.2.1 langchain-text-splitters-0.2.0 langsmith-0.1.63 orjson-3.10.3 packaging-23.2\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.1.7-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.1.46 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.2.1)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (1.30.3)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.46->langchain_openai) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.46->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.46->langchain_openai) (0.1.63)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.46->langchain_openai) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.46->langchain_openai) (2.7.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.46->langchain_openai) (8.3.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (4.11.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain_openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain_openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.46->langchain_openai) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.46->langchain_openai) (3.10.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.46->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.46->langchain_openai) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.0.7)\n",
            "Installing collected packages: tiktoken, langchain_openai\n",
            "Successfully installed langchain_openai-0.1.7 tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "# í™˜ê²½ë³€ìˆ˜ì— OpenAI API í‚¤ ì €ì¥ (ì‚¬ìš©ì ì…ë ¥ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ)\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAIFMWfjlnNa",
        "outputId": "d602b3ec-8cc4-442f-a1c7-83fe79184462"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-4 ì±—ë´‡ ê°ì²´ ìƒì„±\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿, íŒŒì„œë¥¼ ê°ê° ìƒì„±í•œ ë’¤ ì±—ë´‡ê³¼ ì²´ì¸ ê²°í•©\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# PROMPT1 - ë‰´ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ ìƒì„± - chain_1ìœ¼ë¡œ ìƒì„±\n",
        "prompt_1 = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are an AI trained to generate insightful questions from a given article.\"\"\"), # í˜ë¥´ì†Œë‚˜ ë¶€ì—¬\n",
        "    (\"user\", \"\"\"\n",
        "    1. Read the article about current Seoul subway news.\n",
        "    2. Find out what is the main incident related with subway in the article.\n",
        "    3. Find the date that this article was issued\n",
        "    4. Find every date-related expression in the article.\n",
        "    5. Compare 3 and 4, and find out the date the incident was occured.\n",
        "    6. Find every expression related to line of subway in the article.\n",
        "    7. Find out which line the incident is about.\n",
        "    8. Create 2 questions to identify the main points of the news (date of the incident, subway line of the incident). The questions should be 5-way multiple choice questions where you have to choose one of the choices from 1 to 5. One of the options must be â€œUnknownâ€ and the selection for the â€œDate of Eventâ€ question must be in the format 2018ë…„ 3ì›” 18ì¼. Hint is provided with every questions\n",
        "\n",
        "    <Example Question>: 'Where did the subway incident occur? (1) Gangnam Station (2) Seongsu Station (3) Suyu Station (4) Ankguk Station (5) Unknown)',\n",
        "    <Example Hint>:''\n",
        "    News Article= {input}\n",
        "    \"\"\"),\n",
        "    (\"system\", \"\"\"Generate 2 questions. All should be in Korean ONLY.\n",
        "\n",
        "Template:\n",
        "Question1: {{}},\n",
        "Hint1: {{}},\n",
        "Question2: {{}},\n",
        "Hint2: {{}}\"\"\")\n",
        "])\n",
        "chain_1 = prompt_1 | llm | output_parser\n",
        "\n",
        "# PROMPT2 ì…ë ¥ - ë‰´ìŠ¤ ê¸°ì‚¬ì— ëŒ€í•´ ì§ˆë¬¸ì˜ ë‹µë³€ ë„ì¶œ - chain_2ìœ¼ë¡œ ìƒì„±\n",
        "prompt_2 = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You read a news article and answer a question accurately based on what you read.\"\"\"), # í˜ë¥´ì†Œë‚˜ ë¶€ì—¬\n",
        "    (\"user\", \"\"\"\n",
        "    You read a news article like this:\n",
        "    1. Read the article about current Seoul subway news.\n",
        "    2. Find out what is the main incident related with subway in the article.\n",
        "    3. Find the date that this article was issued\n",
        "    4. Find every date-related expression in the article.\n",
        "    5. Compare 3 and 4, and find out the date the incident was occured.\n",
        "    6. Find every expression related to line of subway in the article.\n",
        "    7. Find out which line the incident is about.\n",
        "    Then you answer a question accurately based on what you read.\n",
        "    Example= â€œ1ë²ˆ, 5ë²ˆ, 4ë²ˆâ€,\n",
        "    news= {input}\n",
        "    Questions= {question}\n",
        "    \"\"\"),\n",
        "    (\"system\", \"\"\"\"Template(MUST FOLLOW): Answer1: {{answer_1}}ë²ˆ, Answer2: {{answer_2}}ë²ˆ\"\"\")\n",
        "])\n",
        "chain_2 = prompt_2 | llm | output_parser\n",
        "\n",
        "# PROMPT3 ì…ë ¥ - ë¸”ë¡œê·¸ê¸€ì— ëŒ€í•´ ì§ˆë¬¸ì˜ ë‹µë³€ ë„ì¶œ - chain_3ìœ¼ë¡œ ìƒì„±\n",
        "prompt_3 = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You read a blog article and answer a question accurately based on what you read\"\"\"), # í˜ë¥´ì†Œë‚˜ ë¶€ì—¬\n",
        "    (\"user\", \"\"\"\n",
        "    You read a blog article like this:\n",
        "    1. Read the blog about current Seoul subway news.\n",
        "    2. Find out what is the main incident related with subway in the article.\n",
        "    3. Find every date-related expression in the article.\n",
        "    4. Find out the date the incident was occured.\n",
        "    5. Find every expression related to line of subway in the article.\n",
        "    6. Find out which line the incident is about.\n",
        "    Then you answer 2 questions accurately based on what you read.\n",
        "    Example= â€œ1ë²ˆ, 5ë²ˆ, 4ë²ˆâ€,\n",
        "    blog= {input}\n",
        "    Questions= {question}\n",
        "    \"\"\"),\n",
        "    (\"system\", \"\"\"Template(MUST FOLLOW): Answer1: {{answer_1}}ë²ˆ, Answer2: {{answer_2}}ë²ˆ\"\"\")\n",
        "])\n",
        "chain_3 = prompt_3 | llm | output_parser\n",
        "\n",
        "\n",
        "\n",
        "def quan_eval(title, date, article, blog):\n",
        "\n",
        "  # news = ì œëª© + ë‚ ì§œ + ë³¸ë¬¸\n",
        "  news = f\"\"\"<ë‰´ìŠ¤ ì œëª©>: {news_title},\n",
        "  <ë‰´ìŠ¤ ìƒì„±ì¼ì>: {news_date},\n",
        "  <ë‰´ìŠ¤ ë³¸ë¬¸>: {news_article}\"\"\"\n",
        "\n",
        "  # news ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ 3ê°œ ìƒì„±\n",
        "  question_list = chain_1.invoke({\"input\": news})\n",
        "\n",
        "  # ì§ˆë¬¸ 3ê°œì— ëŒ€í•´ news ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€\n",
        "  answer_list_news = chain_2.invoke({\"input\": news, \"question\": question_list})\n",
        "\n",
        "  # ì§ˆë¬¸ 3ê°œì— ëŒ€í•´ blog ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€\n",
        "  answer_list_blog = chain_3.invoke({\"input\": blog, \"question\": question_list})\n",
        "\n",
        "  # ì¼ì¹˜ ì—¬ë¶€ íŒë‹¨(True/False)\n",
        "  return answer_list_news == answer_list_blog"
      ],
      "metadata": {
        "id": "G_kvYLxpm0RO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ì ìš© ì˜ˆì‹œ**\n",
        "## **ë°ì´í„° ì¤€ë¹„**"
      ],
      "metadata": {
        "id": "gxauSQUdnJs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [ë‰´ìŠ¤]\n",
        "\n",
        "# ì œëª©\n",
        "news_title = '\"ì „ìŸë‚œ ì¤„ ì•Œì•˜ë‹¤\"â€¦ì¶œê·¼ê¸¸ ì§ì¥ì¸ ìš¸ë¦° ì„œìš¸ ì§€í•˜ì²  ìƒí™©'\n",
        "# ë‚ ì§œ\n",
        "news_date = '2023-07-14 00:00:00'\n",
        "# ë³¸ë¬¸\n",
        "news_article = \"\"\"\\ìˆ˜ì¸ë¶„ë‹¹ì„  ì™•ì‹­ë¦¬ì—­ì—ì„œ ì—´ì°¨ë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ì‹œë¯¼ë“¤ì´ ëŒ€ê±° ëª°ë ¤ìˆëŠ” ëª¨ìŠµ. /ì‚¬ì§„=ê¹€ì„¸ë¦° ê¸°ì 14ì¼ ì„œìš¸ì— ë§ì€ ë¹„ê°€ ë‚´ë¦¬ë©´ì„œ ì¼ë¶€ ë„ë¡œì˜ ì¶œì…ì´ ì „ë©´ í†µì œëœ ê°€ìš´ë°, ì¶œê·¼ê¸¸ ì§€í•˜ì² ë¡œ ëª°ë¦° ì‹œë¯¼ë“¤ì´ ê³³ê³³ì—ì„œ ë¶ˆí¸í•¨ì„ í˜¸ì†Œí–ˆë‹¤. ì´ë‚  ì˜¤ì „ 4ì‹œ10ë¶„ê»˜ ì¶œì…ì´ í†µì œëœ ë™ë¶€ê°„ì„ ë„ë¡œ ì „ êµ¬ê°„(ìˆ˜ë½ì§€í•˜ì°¨ë„~ì„±ìˆ˜JC)ì€ ì˜¤ì „ 6ì‹œ 40ë¶„ê»˜ í†µí–‰ì´ ì¬ê°œëìœ¼ë‚˜, ì˜¤ì „ 7ì‹œ 15ë¶„ê»˜ë¶€í„°ëŠ” ì˜¬ë¦¼í”½ëŒ€ë¡œ(ì–‘ë°©í–¥) ì—¬ì˜ìƒë¥˜IC êµí†µ í†µì œê°€ ì‹œì‘ëë‹¤. ì„œìš¸ì‹œ ì¬ë‚œì•ˆì „ëŒ€ì±…ë³¸ë¶€ëŠ” ì‹œë¯¼ë“¤ì—ê²Œ ë¯¸ë¦¬ ë„ë¡œ êµí†µ ìƒí™©ì„ í™•ì¸í•˜ê³ , ê°€ê¸‰ì  ëŒ€ì¤‘êµí†µì„ ì´ìš©í•´ ë‹¬ë¼ê³  ë‹¹ë¶€í–ˆë‹¤. ì´ì— ì¼ë¶€ ì§€í•˜ì²  ë…¸ì„ ì— ì‚¬ëŒì´ ëŒ€ê±° ëª°ë¦¬ê²Œ ëœ ê²ƒìœ¼ë¡œ í’€ì´ëœë‹¤. ìˆ˜ì„œì—­ì—ì„œ ì™•ì‹­ë¦¬ì—­ ë°©í–¥ ì—´ì°¨ ì§€ì—°ìœ¼ë¡œ ëŒ€ê¸° ì¤‘ì´ë˜ ì‹œë¯¼ë“¤ì´ ì—´ì°¨ íƒ‘ìŠ¹ì„ ê¸°ë‹¤ë¦¬ê³  ìˆë‹¤. /ì‚¬ì§„=ê¹€ì„¸ë¦° ê¸°ì ì´ë‚  ì˜¤ì „ 8ì‹œê»˜ ìˆ˜ì¸ ë¶„ë‹¹ì„  ìˆ˜ì„œì—­ì—ì„œ ì™•ì‹­ë¦¬ë¡œ í–¥í•˜ëŠ” ì—´ì°¨ëŠ” ìš´í–‰ì´ 10~15ë¶„ê°€ëŸ‰ ì§€ì—°ë¼ ë‹¤ìŒ ì—­ì¸ ëŒ€ëª¨ì‚°ì…êµ¬ì—­ì—ì„œ ê¸°ë‹¤ë¦¬ë˜ ìŠ¹ê°ë“¤ì´ ëŒ€ê¸°í•´ì•¼ í–ˆë‹¤. ì´í›„ ì¬ê°œëœ ì—´ì°¨ëŠ” 5ëŒ€ê°€ëŸ‰ì´ ì—°ì´ì–´ ëª°ë ¤ì˜¤ëŠ” ìƒí™©ì´ ë°œìƒí–ˆìœ¼ë©°, ë„ì°©í•œ ì—´ì°¨ ë‚´ë¶€ì—” ì‚¬ëŒë“¤ì´ ì´ë¯¸ ê½‰ ë“¤ì–´ì„œ í•œë•Œ ì›í™œí•œ íƒ‘ìŠ¹ì´ ì–´ë ¤ì› ë‹¤. ì´ êµ¬ê°„ ì¢…ì°©ì—­ì¸ ì™•ì‹­ë¦¬ì—ì„œë„ ì²­ëŸ‰ë¦¬ ë°©ë©´ ì—´ì°¨ íƒ‘ìŠ¹ì„ ìœ„í•´ ëŒ€ê¸° ì¤‘ì¸ ì‚¬ëŒë“¤ì´ ë¹¼ê³¡í•˜ê²Œ ì¤„ì§€ì–´ ë“¤ì–´ì„œ ìˆì—ˆë‹¤. ì‹œë¯¼ë“¤ ì‚¬ì´ì—ì„œëŠ” \"ì „ìŸ ë‚œ ê±° ì•„ë‹ˆëƒ\", \"ì›€ì§ì¼ ìˆ˜ê°€ ì—†ë‹¤\", \"ì˜¤ëŠ˜ ì•ˆì— ê°€ëŠ” ê²ƒ ë§ëƒ\"ëŠ” ë“±ì˜ ëª©ì†Œë¦¬ê°€ í˜ëŸ¬ë‚˜ì™”ë‹¤. ì§ì¥ì¸ ì†¡ëª¨ ì”¨(42)ëŠ” \"ë¹„ê°€ ë§ì´ ì™€ì„œ ë„ë¡œ ìƒí™©ì´ ì•ˆ ì¢‹ì€ ê²ƒ ê°™ì•„ ì§€í•˜ì² ì„ íƒ€ëŸ¬ ì˜¨ ê±´ë° ì´ê²Œ ë¬´ìŠ¨ ì¼ì¸ì§€ ë‹¹í™©ìŠ¤ëŸ½ë‹¤\"ë©° \"íšŒì‚¬ ê°€ëŠ” ê±´ ì´ë¯¸ ì§€ê°ì´ê³ , ì§€ì˜¥ì² ì´ë¼ì„œ ë¶ˆí¸í•  ê²ƒ ê°™ë‹¤\"ê³  í† ë¡œí–ˆë‹¤. ì§€í•˜ì²  ìš´í–‰ì´ í­ìš°ë¡œ ì¸í•´ ëŠ¦ì–´ì§„ ê²ƒê³¼ ê´€ë ¨, ì§ì¥ì¸ ì˜¨ë¼ì¸ ì»¤ë®¤ë‹ˆí‹° 'ë¸”ë¼ì¸ë“œ'ì—ì„œë„ \"ì˜¤ëŠ˜ 2í˜¸ì„  ì—­ë§ˆë‹¤ ëª‡ë¶„ì”© ì •ì°¨í•˜ëŠ” ê²ƒì´ëƒ\", \"ì¶œê·¼ ì‹œê°„ì— ì§€í•˜ì²  ì™œ ì´ëŸ¬ëŠ” ê±°ëƒ\", \"ì˜¤ëŠ˜ ì§€í•˜ì²  ì™œ ì´ëŸ¬ëƒ ì§€ê° ë‹¹ì²¨ì´ë‹¤\" ë“±ì˜ ë°˜ì‘ì´ ë‚˜ì™”ë‹¤. ì§‘ì¤‘í˜¸ìš°ê°€ ë‚´ë¦° 14ì¼ ì˜¤ì „ ì„œìš¸ ì ìˆ˜êµê°€ ê°•ë¬¼ì— ì ê²¨ í†µì œë˜ê³  ìˆë‹¤. /ì‚¬ì§„=ë‰´ìŠ¤1 ì´ì™¸ì—ë„ í­ìš°ë¡œ ì¸í•œ í”¼í•´ë„ ê³„ì†ëœë‹¤. ë°¤ì‚¬ì´ ì„¸ì°¬ ë¹„ê°€ ì´ì–´ì§€ë©´ì„œ ì„œìš¸ 2ê°œ êµ¬ 4000ì—¬ì„¸ëŒ€ì—ì„œ ì •ì „ í”¼í•´ê°€ ë°œìƒí–ˆìœ¼ë©°, ì „êµ­ 6ê°œ ì‹œë„ 21ê°œ ì‹œêµ°êµ¬ 134ëª…ì´ ì¼ì‹œ ëŒ€í”¼í–ˆë‹¤. í•œí¸ ê¸°ìƒì²­ì— ë”°ë¥´ë©´ ì˜¤ëŠ˜ê¹Œì§€ ì„œìš¸ê³¼ ì¸ì²œ, ê²½ê¸° ë¶ë¶€, ê°•ì›ì¤‘Â·ë¶ë¶€ ë‚´ë¥™Â·ì‚°ì§€ì— ëŒí’ê³¼ ì²œë‘¥Â·ë²ˆê°œë¥¼ ë™ë°˜í•œ ì‹œê°„ë‹¹ 30âˆ¼80mmì˜ ë§¤ìš° ê°•í•œ ë¹„ê°€ ë‚´ë¦°ë‹¤. í–‰ì •ì•ˆì „ë¶€ëŠ” ìœ„ê¸° ê²½ë³´ ìˆ˜ì¤€ì„ 'ê²½ê³„'ì—ì„œ ìµœê³  ìˆ˜ì¤€ì¸ 'ì‹¬ê°' ë‹¨ê³„ë¡œ ìƒí–¥í•˜ê³  ì¤‘ì•™ì¬ë‚œì•ˆì „ëŒ€ì±…ë³¸ë¶€(ì¤‘ëŒ€ë³¸) 2ë‹¨ê³„ë¥¼ 3ë‹¨ê³„ë¡œ ì˜¬ë¦¬ê¸°ë¡œ í–ˆë‹¤.\"\"\" # ë³¸ë¬¸\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# [ë¸”ë¡œê·¸]\n",
        "\n",
        "blog_sample = \"\"\"\n",
        "## \"í­ìš°ì— ë§‰íŒ ì„œìš¸ì˜ ì•„ì¹¨: ì§€í•˜ì²  ì—°ì°©ìœ¼ë¡œ ì¶œê·¼ê¸¸ ë’¤ìˆ­ìˆ­\"\n",
        "\n",
        "## ì‹œì‘í•˜ëŠ” ë§:\n",
        "ì•ˆë…•í•˜ì„¸ìš”, ì—¬ëŸ¬ë¶„ì˜ ì¶œí‡´ê·¼ ë©”ì‹ ì € ì§€í•˜ì²  ì˜¨ë‹¤ì˜ 'ì˜¤.ì§€.í†µ [ì˜¤ëŠ˜ì˜ ì§€í•˜ì²  ì†Œì‹í†µ]' ì¸ì‚¬ ë“œë¦½ë‹ˆë‹¤!\n",
        "\n",
        "-{ì§€ì—°/ì‚¬ê³  ì¼ì‹œ}: 2023ë…„ 7ì›” 14ì¼ ì˜¤ì „\n",
        "-{ì§€ì—°/ì‚¬ê³  ë…¸ì„ }: ìˆ˜ì¸ë¶„ë‹¹ì„ \n",
        "-{ì§€ì—°/ì‚¬ê³  ì´ìœ }: ê°•í•œ í­ìš°ë¡œ ì¸í•œ ë…¸ì„  ì¼ë¶€ êµ¬ê°„ì˜ ë„ë¡œ ì¶œì… í†µì œ ë° ëŒ€ì²´ ëŒ€ì¤‘êµí†µ ìˆ˜ìš” ê¸‰ì¦\n",
        "-{ë¬¸ì˜ ì‚¬í•­}: [ì„œìš¸êµí†µê³µì‚¬ í™ˆí˜ì´ì§€](https://www.seoulmetro.co.kr)\n",
        "\n",
        "## ê°„ë‹¨í•œ ìš”ì•½ê¸€:\n",
        "ì§€ë‚œ 14ì¼, ì„œìš¸ì„ í¬í•¨í•œ ìˆ˜ë„ê¶Œì— ë‚´ë¦° ì§‘ì¤‘ í˜¸ìš°ë¡œ ì¸í•´ ë§ì€ ë„ë¡œê°€ í†µì œë˜ì—ˆê³ , ëŒ€ì¤‘êµí†µ ì´ìš©ì— í° ì°¨ì§ˆì´ ë¹šì–´ì¡ŒìŠµë‹ˆë‹¤. íŠ¹íˆ ìˆ˜ì¸ë¶„ë‹¹ì„ ì—ì„œëŠ” ìˆ˜ì„œì—­ì—ì„œ ì™•ì‹­ë¦¬ì—­ ë°©í–¥ìœ¼ë¡œ í–¥í•˜ëŠ” ì—´ì°¨ê°€ í‰ì†Œë³´ë‹¤ 10~15ë¶„ ê°€ëŸ‰ ì§€ì—°ë˜ì—ˆìœ¼ë©°, ì´ë¡œ ì¸í•´ ìŠ¹ê°ë“¤ì´ ì—´ì°¨ ë‚´ë¶€ì—ì„œ ê¸´ ëŒ€ê¸° ì‹œê°„ì„ ê²ªì–´ì•¼ í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ì²­ëŸ‰ë¦¬ ë°©í–¥ìœ¼ë¡œ ê°€ëŠ” ì—´ì°¨ë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ìŠ¹ê°ë“¤ë¡œ ì¸í•´ ì™•ì‹­ë¦¬ì—­ì´ ë¹¼ê³¡íˆ ë“¤ì–´ì°¨ ì›í™œí•œ íƒ‘ìŠ¹ì´ ì–´ë ¤ìš´ ìƒí™©ì´ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ìƒí™©ì€ ì¶œê·¼ ì‹œê°„ëŒ€ì— ì§ì¥ì¸ë“¤ ì‚¬ì´ì—ì„œ í° ë¶ˆí¸ì„ ì´ˆë˜í–ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "## ë§ˆë¬´ë¦¬ ë§:\n",
        "ì˜¤ì§€í†µì´ ì‹¤ì‹œê°„ìœ¼ë¡œ ë‹¤ì–‘í•œ ì§€í•˜ì²  ì •ë³´ë¥¼ ì—…ë°ì´íŠ¸ í•  ì˜ˆì •ì´ë‹ˆ, ìì£¼ ë°©ë¬¸í•´ ì£¼ì„¸ìš”. 'ì§€í•˜ì²  ì˜¨ë‹¤'ëŠ” ë‹¨ í•œ ë²ˆì˜ í„°ì¹˜ë¡œ ìì‹ ì˜ ìœ„ì¹˜ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ì§€í•˜ì² ì—­ì˜ ì‹¤ì‹œê°„ ë„ì°© ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì•„ì¹¨ë§ˆë‹¤ ì´ëŸ° ì‚¬íƒœë¡œ ì§€ê°ì´ ê±±ì •ë˜ì‹œì£ ? ìš°ë¦¬ í•¨ê»˜ ì˜ ëŒ€ë¹„í•´ë´…ì‹œë‹¤!\n",
        "\n",
        "ğŸ”½ ì§€í•˜ì²  ì˜¨ë‹¤ ì†Œê°œ ë³´ëŸ¬ê°€ê¸°\n",
        "\n",
        "https://blog.naver.com/subway__onda/223258646349\"\"\""
      ],
      "metadata": {
        "id": "WOMTceL8nHfI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quan_eval(news_title, news_date, news_article, blog_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLlndUWTnKQo",
        "outputId": "1f4cda83-86c1-449c-e1dd-1e808534e460"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}