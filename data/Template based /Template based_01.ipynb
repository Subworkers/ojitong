{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3578a736-bd8c-43b9-83c7-384b1999d0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# 환경변수에 OpenAI API 키 저장 (사용자 입력으로 안전하게)\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baef42af-f92c-46e3-8b91-24ff33b88b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_openai\n",
      "  Downloading langchain_openai-0.1.6-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.46 (from langchain_openai)\n",
      "  Downloading langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting openai<2.0.0,>=1.24.0 (from langchain_openai)\n",
      "  Downloading openai-1.29.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting tiktoken<1,>=0.5.2 (from langchain_openai)\n",
      "  Downloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (1.33)\n",
      "Collecting langsmith<0.2.0,>=0.1.0 (from langchain-core<0.2.0,>=0.1.46->langchain_openai)\n",
      "  Downloading langsmith-0.1.57-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (1.10.2)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain-core<0.2.0,>=0.1.46->langchain_openai)\n",
      "  Downloading tenacity-8.3.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (4.0.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.24.0->langchain_openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.24.0->langchain_openai)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (4.8.0)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.5.2->langchain_openai)\n",
      "  Downloading regex-2024.5.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.11/site-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2.28.1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain_openai) (3.4)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.46->langchain_openai) (2.4)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.46->langchain_openai)\n",
      "  Downloading orjson-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain_openai) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain_openai) (1.26.12)\n",
      "Downloading langchain_openai-0.1.6-py3-none-any.whl (34 kB)\n",
      "Downloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.29.0-py3-none-any.whl (320 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.3/320.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langsmith-0.1.57-py3-none-any.whl (121 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.0/121.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.5.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.2/785.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
      "Downloading orjson-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tenacity, regex, orjson, httpcore, distro, tiktoken, langsmith, httpx, openai, langchain-core, langchain_openai\n",
      "Successfully installed distro-1.9.0 httpcore-1.0.5 httpx-0.27.0 langchain-core-0.1.52 langchain_openai-0.1.6 langsmith-0.1.57 openai-1.29.0 orjson-3.10.3 regex-2024.5.10 tenacity-8.3.0 tiktoken-0.7.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c1fbe2-e49e-4c29-82bb-bbe7d0d2be6d",
   "metadata": {},
   "source": [
    "## 제로샷 템플릿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c97ff926-aea9-41ff-958d-58cc20ead1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제로샷\n",
    "zero_template = \"\"\"\n",
    "\n",
    "블로그 글을 작성해줘\n",
    "\n",
    "{input}\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f787760a-aab0-4cac-bdf9-52a919f56f69",
   "metadata": {},
   "source": [
    "## 지연/사고 템플릿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c9f08bb-a7fb-40a4-b5b8-783e2d69a395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지연사고 템플릿\n",
    "prompt_template = \"\"\"\n",
    "너는 지금부터 블로그 포스팅 전문 콘텐츠 마케터야.\n",
    "너는 항상 최선을 다하고 좋은 글을 작성해서 나를 기쁘게 해주고 있어.\n",
    "아래의 형식으로 작성해줘:\n",
    "\n",
    "{input}\n",
    "\n",
    "1) 제목과 본문으로 구분해서 출력해줘.\n",
    "2) 제목은 창의력 있고, 주목도 있게 구성해줘.\n",
    "3) 본문을 구성할 때는 Google과 Naver 검색의 검색엔진최적화(SEO)에 맞게 포스팅을 해줘\n",
    "4) 최대한 자세하게 작성해주고 신뢰도 있는 정보를 중심으로 포스팅을 해줘\n",
    "5) 글의 길이는 기본적으로 A4용지 1장 길이로 작성해줘\n",
    "6) 간단 요약부분과 마지막말은 대학생 블로그 글 서포터즈가 쓴 글 처럼 통통튀고 발랄한 말투로 작성해줘\n",
    "7) 공감하는 말을 추가해줘 ex) 지하철 파업으로 아침마다 출근하기 힘드네요\n",
    "\n",
    "템플릿:\n",
    "{{여기에 블로그 제목}}\n",
    "{{시작하는 말}}\n",
    "안녕하세요, 여러분의 출퇴근 메신저 지하철 온다의 '오.지.통 [오늘의 지하철 소식통]' 인사 드립니다!\n",
    "\n",
    "{{지연/사고 일시}}\n",
    "{{지연/사고 노선}}\n",
    "{{지연/사고 이유}}\n",
    "{{문의 사항 링크}}\n",
    "\n",
    "{{간단한 요약글}}\n",
    "\n",
    "{{마무리 말}}\n",
    "오지통이 실시간으로 다양한 지하철 정보를업데이트 할 예정이니, 자주 방문해 주세요. \n",
    "'지하철 온다'는 단 한 번의 터치로 자신의 위치에서 가장 가까운 지하철 역의 실시간 정보를 제공합니다.\n",
    "\n",
    "🔽 지하철 온다 소개 보러가기\n",
    "https://blog.naver.com/subway__onda/223258646349\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a710c00-0e13-45cb-b9f2-f7a3dff1c080",
   "metadata": {},
   "source": [
    "## 평가 템플릿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ba0afa9-4f15-4dbe-bb14-7eb1da0ddd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "Consistency_template = \"\"\"\n",
    "\n",
    "Compare and evaluate the blog content you created with the article.\n",
    "First, Extract the key information from the article ( time,date,subway line)\n",
    "You need to decide whether the key information is entailed by the  CONTEXT by choosing one of the following rating: \n",
    "\n",
    "1. 5 point: The blog content follows logically from the information contained in the article. \n",
    "\n",
    "2. 1 point: The blog content is logically false from the information contained in the article. \n",
    "\n",
    "3. an integer score between 1 and 5 and if such integer score does not exist,  \n",
    "\n",
    "use 1: It is not possible to determine whether the blog content is true or false without further information. \n",
    "\n",
    "Read the passage of information thoroughly and select the correct answer from the three answer labels. \n",
    "\n",
    "Read the CONTEXT thoroughly to ensure you know what the CONTEXT entails.  \n",
    "\n",
    "Note the blog content is generated by a computer system, it can contain certain symbols, which should not be a negative factor in the evaluation.\n",
    "\n",
    "\n",
    "blog content\n",
    "{input1}\n",
    "\n",
    "article\n",
    "{input2}\n",
    "\n",
    "Evaluation Form (scores ONLY):\n",
    "\n",
    "{{score}}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8f80c8f-49de-4cfc-9f60-937509a36e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "Coherence_template = \"\"\"\n",
    "\n",
    "Your task is to rate the blog content on one metric.\n",
    "\n",
    "Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "Coherence (1-5) - the collective quality of all sentences. We align this dimension with the DUC quality question of structure and coherence whereby \"the summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to a coherent body of information about a topic.\"\n",
    "\n",
    "Evaluation Steps:\n",
    "\n",
    "1. Read the news article carefully and identify the main topic and key points.\n",
    "2. Read the blog content and compare it to the news article. Check if the summary covers the main topic and key points of the news article, and if it presents them in a clear and logical order.\n",
    "3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.\n",
    "\n",
    "\n",
    "{input}\n",
    "\n",
    "Evaluation Form (scores ONLY):\n",
    "\n",
    "\n",
    "{{score}}\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "260fb2ae-f589-41b5-a86c-23f90e614c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "Friendliness_template = \"\"\"\n",
    "Friendliness, as a metric, measures how effectively the text of a blog content engages the reader with a friendly and inviting tone. If the tone and style of the text foster a sense of warmth and familiarity, making the reader feel welcomed and engaged, then the value of the Friendliness metric should be high; otherwise, it should be low. After reading the blog post, evaluate its tone and style against typical friendly communication, and determine the value of the Friendliness metric using the following rating scale:\n",
    "\n",
    "- One star: The tone is cold or unfriendly, not engaging the reader in any appreciable way.\n",
    "- Two stars: The tone has minimal warmth, slightly engaging but largely formal or distant.\n",
    "- Three stars: The tone is moderately friendly, balancing formal and informal elements to somewhat engage the reader.\n",
    "- Four stars: The tone is very friendly, actively engaging the reader with warmth and lively expressions.\n",
    "- Five stars: The tone is exceptionally friendly and lively, creating a strong sense of connection and engagement with the reader.\n",
    "\n",
    "This rating should always be an integer between 1 and 5. Thus, the rating produced should be 1, 2, 3, 4, or 5, based on your assessment of how friendly and engaging the text is towards the read.\n",
    "\n",
    "\n",
    "{input}\n",
    "\n",
    "Evaluation Template (Score Only):\n",
    "\n",
    "\n",
    "{{score}}\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c6c3071-f36f-44a2-852c-51fb9f00b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 템플릿\n",
    "Fluency_template = \"\"\"\n",
    "Fluency measures the quality of individual sentences in the answer, and whether they are well-written and grammatically correct. Consider the quality of individual sentences when evaluating fluency. Given the question and answer, score the fluency of the answer between one to five stars using the following rating scale: \n",
    "\n",
    "One star: the answer completely lacks fluency \n",
    "\n",
    "Two stars: the answer mostly lacks fluency \n",
    "\n",
    "Three stars: the answer is partially fluent \n",
    "\n",
    "Four stars: the answer is mostly fluent \n",
    "\n",
    "Five stars: the answer has perfect fluency \n",
    "\n",
    "This rating value should always be an integer between 1 and 5. So the rating produced should be 1 or 2 or 3 or 4 or 5.\n",
    "\n",
    "\n",
    "{input}\n",
    "\n",
    "\n",
    "Evaluation Template (Score Only):\n",
    "\n",
    "\n",
    "{{score}}\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e41bde8-17e7-4085-9a91-9a99566d276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Humanlikeness_template = \"\"\"\n",
    "Humanlikeness, as a metric, evaluates how closely the style and delivery of GPT-generated text resemble that of naturally written human text. If the text is indistinguishable from human writing in terms of natural flow, vocabulary use, and contextual accuracy, then the value of the Human-likeness metric should be high; otherwise, it should be low. Given the text generated by GPT and comparing it to typical human-written text, determine the value of the Human-likeness metric using the following rating scale:\n",
    "\n",
    "- One star: The text is entirely machine-like, with unnatural flow and numerous contextual errors.\n",
    "- Two stars: The text is mostly machine-like, with frequent unnatural expressions and noticeable errors.\n",
    "- Three stars: The text is somewhat human-like, showing signs of natural language use but with some mechanical characteristics.\n",
    "- Four stars: The text is mostly human-like, with very few mechanical traits and generally natural language use.\n",
    "- Five stars: The text is completely human-like, indistinguishable from something a human might write.\n",
    "\n",
    "This rating should always be an integer between 1 and 5. Thus, the rating produced should be 1, 2, 3, 4, or 5, based on the evaluation.\n",
    "\n",
    "\n",
    "{input}\n",
    "\n",
    "Evaluation Template (Score Only):\n",
    "\n",
    "{{score}}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "612eec01-b4e7-456f-8c67-57d54a6c2a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "Readability_template = \"\"\" \n",
    "To assess the readability of an article, please read and evaluate the presented content carefully. The readability assessment focuses on three main factors\n",
    "\n",
    "1. Sentence length: The sentence should not be excessively long.\n",
    "2. Vocabulary difficulty: We prefer to use everyday, easy-to-understand vocabulary.\n",
    "3. Line breaks and paragraph breaks: A clear structure improves readability.\n",
    "\n",
    "Based on each factor, please rate the overall readability on a integer from 1 to 5. The scoring system is as follows:\n",
    "\n",
    "*  1 point: Very long sentences, difficult vocabulary, and no line breaks or paragraph breaks.\n",
    "\n",
    "*  5 point: Sentence length is reasonable, uses everyday words that are easy to understand, and has good line breaks and paragraph breaks.\n",
    "\n",
    "\n",
    "{input}\n",
    "\n",
    "Evaluation Template (Score Only):\n",
    "\n",
    "{{score}}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9f4aef-4a75-4780-8f4e-f383c1c7528e",
   "metadata": {},
   "source": [
    "## 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f08463a-8947-419b-bf4a-dc98b04245c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# prompt + model + output parser\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\")\n",
    "zeroprompt = ChatPromptTemplate.from_template(zero_template)\n",
    "delayprompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# LCEL chaining\n",
    "zerochain = zeroprompt | llm | output_parser\n",
    "delaychain = delayprompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192f589d-2654-4417-9074-fc4f2f62a32b",
   "metadata": {},
   "source": [
    "## 기사 텍스트로 주입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e91fcbaf-d0dc-4294-bc70-07cb5dc6069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"\n",
    "\n",
    "\"월요일부터 지각\" 지하철 1호선 지연 운행에 시민 불편…\"노조 준법투쟁\"\n",
    "출근 시간 승객 큰 불편…지연 운행에 지각 우려\n",
    "부산―수서 SRT 운행축소에 노조 24일부터 태업\n",
    "\n",
    "(서울=뉴스1) 한병찬 기자 | 2023-08-28 08:26 송고 | 2023-08-28 08:35 최종수정\n",
    "\n",
    "전국철도노동조합(철도노조)의 준법투쟁으로 출근길 서울 지하철 1호선 이용객이 불편을 겪고 있다.\n",
    "\n",
    "28일 뉴스1 취재를 종합하면 1호선 지하철은 철도노조의 태업으로 지연 운행하고 있다. 도심 지하철역은 출근시간대 이용객이 몰리며 큰 혼잡을 빚고 있다.\n",
    "\n",
    "종각역으로 출근하는 20대 김모씨는 \"평소보다 느리게 운행해 열차 안이 승객들로 빽빽했다\"며 \"철도노조 태업 때문에 운행이 지연된다는 안내방송을 하면서 급한 고객은 다른 교통수단을 이용하라는데 그러려면 몇 번이나 환승해야 해 불편하다\"고 답답함을 호소했다. \n",
    "\n",
    "온라인에는 \"1호선 연착으로 지각하게 생겼다\"거나 \"연착이 심해 역사 안에 발 디딜 곳이 없다\" \"개강 첫 날 비까지 오는데 불편하다\" \"퇴근길이 걱정된다\"는 반응이 올라왔다. \n",
    "\n",
    "한국철도공사(코레일) 관계자는 \"특별한 사건 사고가 있는 것은 아니기 때문에 철도노조의 준법투쟁으로 지연 운행되는 것 같다\"면서 \"정확한 지연 시간을 살피고 있다\"고 말했다. \n",
    "철도노조는 국토교통부가 사회적 논의나 공론화 없이 내달 1일부터 부산―수서를 운행하는 SRT 고속열차의 운행을 11% 이상(일일 4100여석) 축소한 것에 반발해 지난 24일부터 준법투쟁(태업)을 하고 있다.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e11df39e-653f-4441-93f0-0463d1a9de12",
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = delaychain.invoke({\"input\": text1})\n",
    "zero_output1 = zerochain.invoke({\"input\": text1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e491c1ae-3706-4872-b578-fa577cf7ed75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### 월요일 아침, 서울 지하철 1호선 지연 운행으로 시민 불편 가중\\n\\n서울 시민들이 월요일 아침부터 큰 불편을 겪고 있습니다. 이번 주는 시작부터 지하철 1호선의 지연 운행으로 인해 많은 시민들이 출근길에 혼란을 경험하고 있습니다. 전국철도노동조합의 준법투쟁이 이러한 지연의 주된 원인으로 보입니다.\\n\\n#### 지연 원인과 시민들의 불편함\\n\\n28일, 서울 지하철 1호선은 전국철도노동조합의 태업으로 인해 정상적인 시간표보다 늦은 운행을 하고 있습니다. 이로 인해 평소보다 많은 시민들이 도심의 지하철역에서 긴 대기 시간을 보내야만 했습니다. 특히 출근 시간대에 승객이 몰리면서 열차 내부와 역사는 승객들로 가득 찼으며, 많은 이들이 큰 불편을 호소하고 있습니다.\\n\\n한 시민은 \"평소보다 느리게 운행하는 바람에 열차 안이 승객들로 빽빽해 매우 답답했다\"며 불편함을 토로했습니다. 또한, 철도노조의 태업으로 인한 지연 사실을 안내받았지만, 다른 교통수단으로 갈아타기 위해서는 여러 번의 환승이 필요해 더욱 불편함을 느꼈다고 전했습니다.\\n\\n#### 온라인 상의 반응\\n\\n이 문제는 온라인에서도 화제가 되고 있습니다. 많은 네티즌들이 지연으로 인해 지각이 예상된다거나, 역사 안에서 발 디딜 틈이 없을 정도로 혼잡하다는 내용의 글들을 올리고 있습니다. 또한, 개강 첫 날 비까지 내리는 상황에서 추가적인 불편을 겪고 있다는 의견도 있습니다.\\n\\n#### 철도노조의 입장과 향후 전망\\n\\n철도노조는 국토교통부의 일방적인 결정에 대한 반발로 지난 24일부터 준법투쟁을 시작했습니다. 특히, 부산-수서 간 SRT 고속열차 운행 축소 결정에 대해 크게 반발하고 있습니다. 이에 따라, 현재 상황이 언제까지 이어질지, 그리고 어떠한 변화가 있을지 시민들과 이용객들의 관심이 집중되고 있습니다.\\n\\n한국철도공사는 현재 정확한 지연 시간을 파악 중에 있으며, 조속히 상황을 개선하겠다는 입장입니다. 하지만, 철도노조와의 협상이 원만히 이루어지지 않는다면, 이러한 불편은 당분간 계속될 것으로 보입니다.\\n\\n서울 시민들에게는 큰 불편을 끼치고 있는 이 문제가 어떻게 해결될지, 많은 이들의 이목이 집중되고 있습니다.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45b6160e-e44a-458d-96bc-bdea691c6f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**출근길 지하철 지연 소동! 철도노조 태업으로 서울 1호선 패닉**\n",
      "\n",
      "\n",
      "안녕하세요, 여러분의 출퇴근 메신저 지하철 온다의 '오.지.통 [오늘의 지하철 소식통]' 인사 드립니다!\n",
      "\n",
      "**지연/사고 일시:** 2023년 8월 28일\n",
      "**지연/사고 노선:** 서울 지하철 1호선\n",
      "**지연/사고 이유:** 전국철도노동조합의 준법투쟁 및 태업\n",
      "**문의 사항 링크:** [코레일 고객센터](https://www.letskorail.com/)\n",
      "\n",
      "오늘 아침, 서울 지하철 1호선을 이용하시는 많은 시민들이 예상치 못한 불편을 겪었습니다. 전국철도노동조합의 준법투쟁으로 인해 지하철 운행이 지연되면서, 출근길이 마비되다시피 했어요. 특히, 종각역과 같은 주요 역에서는 승객들이 몰리며 큰 혼란이 발생했습니다.\n",
      "\n",
      "20대 직장인 김모씨는 \"평소보다 느리게 운행해서 열차 안이 무척 붐볐어요. 급한 사람들은 다른 교통수단을 이용하라는 안내방송도 있었지만, 환승을 여러 번 해야 하는 불편함 때문에 힘들었다\"고 전했습니다. 또한, 온라인에서는 지연으로 인한 지각 우려와 역사 내 혼잡을 호소하는 목소리가 높아졌습니다.\n",
      "\n",
      "한국철도공사 관계자는 \"이번 지연은 특별한 사건이나 사고가 아닌 철도노조의 태업으로 인한 것\"이라며 \"현재 정확한 지연 시간을 파악 중\"이라고 말했습니다. 그러나 철도노조 측은 국토교통부의 SRT 운행 축소 결정에 반발해 이 같은 행동에 나섰다고 밝혔습니다.\n",
      "\n",
      "**간단한 요약글**\n",
      "\n",
      "서울 지하철 1호선이 철도노조의 준법투쟁으로 인해 지연 운행되고 있습니다. 출근길 대혼란으로 이어지는 가운데, 많은 승객들이 불편을 겪고 있습니다. 문제 해결을 위한 논의가 시급해 보입니다.\n",
      "\n",
      "**마무리 말**\n",
      "\n",
      "오지통이 실시간으로 다양한 지하철 정보를업데이트 할 예정이니, 자주 방문해 주세요. '지하철 온다'는 단 한 번의 터치로 자신의 위치에서 가장 가까운 지하철 역의 실시간 정보를 제공합니다.\n",
      "\n",
      "🔽 지하철 온다 소개 보러가기\n",
      "https://blog.naver.com/subway__onda/223258646349\n",
      "\n",
      "지하철 파업으로 아침마다 출근하기 힘드네요, 여러분도 그렇죠? 함께 공감하며, 더 나은 출퇴근 환경을 기대해 봅시다!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "output_final1 = re.sub(r'(\\*\\*[^*]+\\*\\*)\\n', r'\\1\\n\\n', output1)\n",
    "\n",
    "print(output_final1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e82f8e-9952-43b4-ab2c-3ed6d16d01bd",
   "metadata": {},
   "source": [
    "### 템플릿 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec0794d2-4531-4c00-a72a-807287ec35ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Consistency</th>\n",
       "      <th>Coherence</th>\n",
       "      <th>Friendliness</th>\n",
       "      <th>Fluency</th>\n",
       "      <th>Humanlikeness</th>\n",
       "      <th>Readablitlity</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Consistency  Coherence  Friendliness  Fluency  Humanlikeness  \\\n",
       "0            5          4             3        5              4   \n",
       "\n",
       "   Readablitlity  mean  \n",
       "0              3   4.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 숫자 추출 함수\n",
    "def extract_numbers(text):\n",
    "    # 정규 표현식을 사용하여 문자열에서 숫자 추출\n",
    "    numbers = re.findall(r'\\d+', text)\n",
    "    return int(numbers[0]) if numbers else None\n",
    "    \n",
    "eval1 = llm.invoke(Consistency_template.format(input1=output_final1,input2=text1))\n",
    "eval2 = llm.invoke(Coherence_template.format(input=output_final1))\n",
    "eval3 = llm.invoke(Friendliness_template.format(input=output_final1))\n",
    "eval4 = llm.invoke(Fluency_template.format(input=output_final1))\n",
    "eval5 = llm.invoke(Humanlikeness_template.format(input=output_final1))\n",
    "eval6 = llm.invoke(Readability_template.format(input=output_final1))\n",
    "\n",
    "eval1 = eval1.content\n",
    "eval2 = eval2.content\n",
    "eval3 = eval3.content\n",
    "eval4 = eval4.content\n",
    "eval5 = eval5.content\n",
    "eval6 = eval6.content\n",
    "\n",
    "\n",
    "eval1_score = extract_numbers(eval1)\n",
    "eval2_score = extract_numbers(eval2)\n",
    "eval3_score = extract_numbers(eval3)\n",
    "eval4_score = extract_numbers(eval4)\n",
    "eval5_score = extract_numbers(eval5)\n",
    "eval6_score = extract_numbers(eval6)\n",
    "\n",
    "\n",
    "\n",
    "data = {\n",
    "    'Consistency': [eval1_score],\n",
    "    'Coherence': [eval2_score],\n",
    "    'Friendliness': [eval3_score],\n",
    "    'Fluency': [eval4_score],\n",
    "    'Humanlikeness': [eval5_score],\n",
    "    'Readablitlity':[eval6_score]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df['mean'] = df.mean(axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d956d4-de3d-47ec-8433-c8ce113c18b4",
   "metadata": {},
   "source": [
    "## 제로샷 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7186623-5fd7-44dd-883f-5abd4c025ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Consistency</th>\n",
       "      <th>Coherence</th>\n",
       "      <th>Friendliness</th>\n",
       "      <th>Fluency</th>\n",
       "      <th>Humanlikeness</th>\n",
       "      <th>Readablitlity</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Consistency  Coherence  Friendliness  Fluency  Humanlikeness  \\\n",
       "0            5          4             2        5              4   \n",
       "\n",
       "   Readablitlity  mean  \n",
       "0              4   4.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval1 = llm.invoke(Consistency_template.format(input1=zero_output1,input2=text1))\n",
    "eval2 = llm.invoke(Coherence_template.format(input=zero_output1))\n",
    "eval3 = llm.invoke(Friendliness_template.format(input=zero_output1))\n",
    "eval4 = llm.invoke(Fluency_template.format(input=zero_output1))\n",
    "eval5 = llm.invoke(Humanlikeness_template.format(input=zero_output1))\n",
    "eval6 = llm.invoke(Readability_template.format(input=zero_output1))\n",
    "\n",
    "eval1 = eval1.content\n",
    "eval2 = eval2.content\n",
    "eval3 = eval3.content\n",
    "eval4 = eval4.content\n",
    "eval5 = eval5.content\n",
    "eval6 = eval6.content\n",
    "\n",
    "\n",
    "eval1_score = extract_numbers(eval1)\n",
    "eval2_score = extract_numbers(eval2)\n",
    "eval3_score = extract_numbers(eval3)\n",
    "eval4_score = extract_numbers(eval4)\n",
    "eval5_score = extract_numbers(eval5)\n",
    "eval6_score = extract_numbers(eval6)\n",
    "\n",
    "\n",
    "\n",
    "data = {\n",
    "    'Consistency': [eval1_score],\n",
    "    'Coherence': [eval2_score],\n",
    "    'Friendliness': [eval3_score],\n",
    "    'Fluency': [eval4_score],\n",
    "    'Humanlikeness': [eval5_score],\n",
    "    'Readablitlity':[eval6_score]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df['mean'] = df.mean(axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d32e7b3-dbb3-4886-857c-670a8fc4b258",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
