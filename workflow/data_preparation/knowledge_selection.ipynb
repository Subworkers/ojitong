{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514ea988-8439-4214-b74e-4bb7c88036a6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    ***REMOVED***,
    "tags": [
     "parameters"
    ***REMOVED***
   ***REMOVED***,
   "outputs": [***REMOVED***,
   "source": [
    "deploy_phase = \"local\"\n",
    "execution_date = '2024-07-06T00:00:00+09:00'"
   ***REMOVED***
  ***REMOVED***,
  {
   "cell_type": "markdown",
   "id": "9cc0f0d0-9f37-498b-bbaf-250d2cb695dd",
   "metadata": {***REMOVED***,
   "source": [
    "# Notebook Initialization"
   ***REMOVED***
  ***REMOVED***,
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee639ee0-9ac0-4534-a9ff-c28b930df9ae",
   "metadata": {***REMOVED***,
   "outputs": [***REMOVED***,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ***REMOVED***
  ***REMOVED***,
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ada3d1-701d-4f61-8446-056c077aecfe",
   "metadata": {***REMOVED***,
   "outputs": [***REMOVED***,
   "source": [
    "import sys"
   ***REMOVED***
  ***REMOVED***,
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d7c152-cdb4-4e15-aff0-8ef2d1c3e4e9",
   "metadata": {***REMOVED***,
   "outputs": [***REMOVED***,
   "source": [
    "# sys.path.insert(0, \"/workspace/ojitong\"***REMOVED***"
   ***REMOVED***
  ***REMOVED***,
  {
   "cell_type": "markdown",
   "id": "3736f512-143d-4f18-a6a4-bcabd6f75708",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    ***REMOVED***,
    "tags": [***REMOVED***
   ***REMOVED***,
   "source": [
    "# Global Const"
   ***REMOVED***
  ***REMOVED***,
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f14bfd-5c03-47bc-8965-8b6e836306b6",
   "metadata": {***REMOVED***,
   "outputs": [***REMOVED***,
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def convert_to_datetime(date_str: str***REMOVED*** -> datetime:\n",
    "    return datetime.fromisoformat(date_str***REMOVED***\n",
    "\n",
    "today_datetime = convert_to_datetime(execution_date***REMOVED***\n",
    "today_datetime"
   ***REMOVED***
  ***REMOVED***,
  {
   "cell_type": "markdown",
   "id": "b5e052fd-205d-405c-8811-e6440d83c345",
   "metadata": {***REMOVED***,
   "source": [
    "# Knowledge Selection"
   ***REMOVED***
  ***REMOVED***,
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37ad65a-a49c-4ae2-91ce-064d5d0d5e88",
   "metadata": {***REMOVED***,
   "outputs": [***REMOVED***,
   "source": [
    "# import onnxruntime as ort\n",
    "# import numpy as np\n",
    "# from kobert import get_onnx_kobert_model\n",
    "# from transformers import BertTokenizer\n",
    "\n",
    "# # ONNX 모델 및 세션 초기화\n",
    "# onnx_path = get_onnx_kobert_model(***REMOVED***\n",
    "# sess = ort.InferenceSession(onnx_path***REMOVED***\n",
    "\n",
    "# import spacy\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Load the Korean model from spaCy\n",
    "# nlp = spacy.load('ko_core_news_lg'***REMOVED***\n",
    "\n",
    "# # Load KoBERT model and tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('monologg/kobert'***REMOVED***\n",
    "\n",
    "# # Define related and action words\n",
    "# related_words = [\n",
    "#     \"지하철\", \"교통\", \"운행\", \"노선\", \"승객\", \"출근\", \"전철\", \"메트로\", \"지하철역\", \"승강장\", \"지하철차량\",\n",
    "#     \"대중교통\", \"교통체계\", \"교통망\", \"환승\", \"전용차선\",\"사고\",\n",
    "# ***REMOVED***\n",
    "\n",
    "# action_words = [\n",
    "#     \"파업\", \"지연\", \"연착\", \"중단\", \"정지\", \"혼잡\", \"운행지연\", \"운행중단\", \"서비스중단\", \"파업예고\",\n",
    "#     \"노동쟁의\", \"노조활동\", \"노사협상\", \"안전점검\", \"사고\", \"충돌\", \"부상\", \"사망\", \"운행변경\",\n",
    "#     \"노선변경\", \"시간표변경\", \"대체교통\", \"운행재개\", \"시위\",\"대피\"\n",
    "# ***REMOVED***\n",
    "\n",
    "# # Function to check dependency and entities\n",
    "# def check_dependency_and_entities(text***REMOVED***:\n",
    "#     doc = nlp(text***REMOVED***\n",
    "#     for token in doc:\n",
    "#         if token.lemma_ in action_words:\n",
    "#             neighborhood = list(token.children***REMOVED*** + list(token.ancestors***REMOVED*** + list(token.subtree***REMOVED***\n",
    "#             for neighbor in neighborhood:\n",
    "#                 if neighbor.lemma_ in related_words:\n",
    "#                     if any(child.dep_ == \"neg\" for child in neighbor.children***REMOVED***:\n",
    "#                         continue\n",
    "#                     return True\n",
    "#                 if neighbor.pos_ in [\"ADP\", \"SCONJ\"***REMOVED***:\n",
    "#                     connected_words = [child.lemma_ for child in neighbor.children***REMOVED***\n",
    "#                     if set(connected_words***REMOVED***.intersection(related_words***REMOVED***:\n",
    "#                         return True\n",
    "#     return False\n",
    "\n",
    "# # Function to get BERT embeddings\n",
    "# def get_bert_embedding(text***REMOVED***:\n",
    "#     # 텍스트를 토크나이즈\n",
    "#     inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True***REMOVED***\n",
    "#     input_ids = inputs['input_ids'***REMOVED***.detach(***REMOVED***.numpy(***REMOVED***\n",
    "#     attention_mask = inputs['attention_mask'***REMOVED***.detach(***REMOVED***.numpy(***REMOVED***\n",
    "#     token_type_ids = inputs['token_type_ids'***REMOVED***.detach(***REMOVED***.numpy(***REMOVED***\n",
    "\n",
    "#     # ONNX 모델을 사용한 추론\n",
    "#     len_seq = input_ids.shape[1***REMOVED***\n",
    "#     outputs = sess.run(\n",
    "#         None,\n",
    "#         {\n",
    "#             'input_ids': input_ids,\n",
    "#             'token_type_ids': token_type_ids,\n",
    "#             'input_mask': attention_mask,\n",
    "#             'position_ids': np.array(range(len_seq***REMOVED******REMOVED***\n",
    "#         ***REMOVED***\n",
    "#     ***REMOVED***\n",
    "\n",
    "#     # 마지막 인코딩 레이어의 출력값 평균을 계산\n",
    "#     last_hidden_state = outputs[-2***REMOVED***  # ONNX 출력 중 마지막 인코딩 레이어에 해당\n",
    "#     mean_embeddings = np.mean(last_hidden_state, axis=1***REMOVED***\n",
    "#     return mean_embeddings\n",
    "\n",
    "# # Function to check topic relevance using BERT embeddings\n",
    "# def check_topics(text, reference_texts***REMOVED***:\n",
    "#     text_embedding = get_bert_embedding(text***REMOVED***\n",
    "#     reference_embeddings = [get_bert_embedding(ref***REMOVED*** for ref in reference_texts***REMOVED***\n",
    "#     similarities = [cosine_similarity(text_embedding, ref_emb***REMOVED***[0***REMOVED***[0***REMOVED*** for ref_emb in reference_embeddings***REMOVED***\n",
    "#     return max(similarities***REMOVED*** > 0.8  # Adjusted threshold for relevance\n",
    "\n",
    "# # Load your dataframe (assuming df is already defined and has a 'text' column***REMOVED***\n",
    "# reference_texts = [\n",
    "#     \"지하철 파업으로 인한 운행 중단\",\n",
    "#     \"지하철 사고 발생\",\n",
    "#     \"지하철 연착 문제\",\n",
    "#     \"지하철 노선 변경\",\n",
    "#     \"지하철 운행 재개\",\n",
    "#     \"지하철 서비스 중단\",\n",
    "#     \"지하철 안전 점검\",\n",
    "#     \"지하철 충돌 사고\",\n",
    "#     \"지하철 승객 혼잡\",\n",
    "#     \"지하철 출근 시간 문제\",\n",
    "#     \"지하철 노조 파업\",\n",
    "#     \"지하철 운행 변경\",\n",
    "#     \"지하철 대체 교통\"\n",
    "# ***REMOVED***\n",
    "\n",
    "# # Check if the document is relevant\n",
    "# def is_relevant_document(title***REMOVED***:\n",
    "#     return check_dependency_and_entities(title***REMOVED*** or check_topics(title, reference_texts***REMOVED***\n",
    "\n",
    "# # Assuming df is already defined and has a 'title' column\n",
    "# df['is_relevant'***REMOVED*** = df['title'***REMOVED***.apply(is_relevant_document***REMOVED***\n",
    "# df = df[df['is_relevant'***REMOVED******REMOVED***"
   ***REMOVED***
  ***REMOVED***,
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15f30ea-2b8e-488f-9053-0b7750dfb4a0",
   "metadata": {***REMOVED***,
   "outputs": [***REMOVED***,
   "source": [
    "# df"
   ***REMOVED***
  ***REMOVED***,
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f4300-7993-41cd-8bc8-29ea168b451c",
   "metadata": {***REMOVED***,
   "outputs": [***REMOVED***,
   "source": [
    "from data.utils.file import generate_s3_path\n",
    "s3_path = generate_s3_path(today_datetime=today_datetime***REMOVED***\n",
    "df = pd.read_csv(s3_path***REMOVED***"
   ***REMOVED***
  ***REMOVED***,
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63db6f3d-b71f-4a99-b3d9-74acf9382034",
   "metadata": {***REMOVED***,
   "outputs": [***REMOVED***,
   "source": [
    "df"
   ***REMOVED***
  ***REMOVED***,
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d221da71-7df2-495e-baf2-f8f3c446f991",
   "metadata": {***REMOVED***,
   "outputs": [***REMOVED***,
   "source": [
    "# 1. title 추출\n",
    "df['indexed_title'***REMOVED*** = df.index.astype(str***REMOVED*** + \": \" + df['title'***REMOVED***\n",
    "titles = df['indexed_title'***REMOVED***.tolist(***REMOVED***"
   ***REMOVED***
  ***REMOVED***,
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0082369d-50e6-4093-b859-fcae3d83f1bd",
   "metadata": {***REMOVED***,
   "outputs": [***REMOVED***,
   "source": [
    "# 2. Selection News 태스크 실행\n",
    "from data.tasks.selection_news_task import SelectionNewsTask\n",
    "task = SelectionNewsTask(***REMOVED***\n",
    "indices = task.execute(titles***REMOVED***"
   ***REMOVED***
  ***REMOVED***,
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661e4707-19e0-4451-95c3-e7c1e27c8f50",
   "metadata": {***REMOVED***,
   "outputs": [***REMOVED***,
   "source": [
    "valid_indices = [i for i in indices if i < len(df***REMOVED******REMOVED***\n",
    "selected_df = df.iloc[valid_indices***REMOVED***"
   ***REMOVED***
  ***REMOVED***,
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c29b39c-7e39-4ee2-b77d-22e5a837932d",
   "metadata": {***REMOVED***,
   "outputs": [***REMOVED***,
   "source": [
    "selected_df"
   ***REMOVED***
  ***REMOVED***,
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd7542-4463-4544-bd39-c6cabe4defec",
   "metadata": {***REMOVED***,
   "outputs": [***REMOVED***,
   "source": [
    "# 3. 파일 S3 업로드\n",
    "s3_path = generate_s3_path(today_datetime=today_datetime***REMOVED***\n",
    "selected_df.to_csv(s3_path, index=False***REMOVED***\n",
    "print(f\"*** Data saved to {s3_path***REMOVED*** ***\"***REMOVED***"
   ***REMOVED***
  ***REMOVED***,
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45272fbf-fde7-4766-8c76-8804fa6e65c8",
   "metadata": {***REMOVED***,
   "outputs": [***REMOVED***,
   "source": [***REMOVED***
  ***REMOVED***
 ***REMOVED***,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel***REMOVED***",
   "language": "python",
   "name": "python3"
  ***REMOVED***,
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   ***REMOVED***,
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  ***REMOVED***
 ***REMOVED***,
 "nbformat": 4,
 "nbformat_minor": 5
***REMOVED***
