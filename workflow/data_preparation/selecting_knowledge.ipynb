{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514ea988-8439-4214-b74e-4bb7c88036a6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "deploy_phase = \"local\"\n",
    "execution_date = '2024-07-06T00:00:00+09:00'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc0f0d0-9f37-498b-bbaf-250d2cb695dd",
   "metadata": {},
   "source": [
    "# Notebook Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee639ee0-9ac0-4534-a9ff-c28b930df9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d7c152-cdb4-4e15-aff0-8ef2d1c3e4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.insert(0, \"/workspace/ojitong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3736f512-143d-4f18-a6a4-bcabd6f75708",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Global Const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f14bfd-5c03-47bc-8965-8b6e836306b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def convert_to_datetime(date_str: str) -> datetime:\n",
    "    return datetime.fromisoformat(date_str)\n",
    "\n",
    "today_datetime = convert_to_datetime(execution_date)\n",
    "today_datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e052fd-205d-405c-8811-e6440d83c345",
   "metadata": {},
   "source": [
    "# Knowledge Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37ad65a-a49c-4ae2-91ce-064d5d0d5e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import onnxruntime as ort\n",
    "# import numpy as np\n",
    "# from kobert import get_onnx_kobert_model\n",
    "# from transformers import BertTokenizer\n",
    "\n",
    "# # ONNX 모델 및 세션 초기화\n",
    "# onnx_path = get_onnx_kobert_model()\n",
    "# sess = ort.InferenceSession(onnx_path)\n",
    "\n",
    "# import spacy\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Load the Korean model from spaCy\n",
    "# nlp = spacy.load('ko_core_news_lg')\n",
    "\n",
    "# # Load KoBERT model and tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "\n",
    "# # Define related and action words\n",
    "# related_words = [\n",
    "#     \"지하철\", \"교통\", \"운행\", \"노선\", \"승객\", \"출근\", \"전철\", \"메트로\", \"지하철역\", \"승강장\", \"지하철차량\",\n",
    "#     \"대중교통\", \"교통체계\", \"교통망\", \"환승\", \"전용차선\",\"사고\",\n",
    "# ]\n",
    "\n",
    "# action_words = [\n",
    "#     \"파업\", \"지연\", \"연착\", \"중단\", \"정지\", \"혼잡\", \"운행지연\", \"운행중단\", \"서비스중단\", \"파업예고\",\n",
    "#     \"노동쟁의\", \"노조활동\", \"노사협상\", \"안전점검\", \"사고\", \"충돌\", \"부상\", \"사망\", \"운행변경\",\n",
    "#     \"노선변경\", \"시간표변경\", \"대체교통\", \"운행재개\", \"시위\",\"대피\"\n",
    "# ]\n",
    "\n",
    "# # Function to check dependency and entities\n",
    "# def check_dependency_and_entities(text):\n",
    "#     doc = nlp(text)\n",
    "#     for token in doc:\n",
    "#         if token.lemma_ in action_words:\n",
    "#             neighborhood = list(token.children) + list(token.ancestors) + list(token.subtree)\n",
    "#             for neighbor in neighborhood:\n",
    "#                 if neighbor.lemma_ in related_words:\n",
    "#                     if any(child.dep_ == \"neg\" for child in neighbor.children):\n",
    "#                         continue\n",
    "#                     return True\n",
    "#                 if neighbor.pos_ in [\"ADP\", \"SCONJ\"]:\n",
    "#                     connected_words = [child.lemma_ for child in neighbor.children]\n",
    "#                     if set(connected_words).intersection(related_words):\n",
    "#                         return True\n",
    "#     return False\n",
    "\n",
    "# # Function to get BERT embeddings\n",
    "# def get_bert_embedding(text):\n",
    "#     # 텍스트를 토크나이즈\n",
    "#     inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "#     input_ids = inputs['input_ids'].detach().numpy()\n",
    "#     attention_mask = inputs['attention_mask'].detach().numpy()\n",
    "#     token_type_ids = inputs['token_type_ids'].detach().numpy()\n",
    "\n",
    "#     # ONNX 모델을 사용한 추론\n",
    "#     len_seq = input_ids.shape[1]\n",
    "#     outputs = sess.run(\n",
    "#         None,\n",
    "#         {\n",
    "#             'input_ids': input_ids,\n",
    "#             'token_type_ids': token_type_ids,\n",
    "#             'input_mask': attention_mask,\n",
    "#             'position_ids': np.array(range(len_seq))\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "#     # 마지막 인코딩 레이어의 출력값 평균을 계산\n",
    "#     last_hidden_state = outputs[-2]  # ONNX 출력 중 마지막 인코딩 레이어에 해당\n",
    "#     mean_embeddings = np.mean(last_hidden_state, axis=1)\n",
    "#     return mean_embeddings\n",
    "\n",
    "# # Function to check topic relevance using BERT embeddings\n",
    "# def check_topics(text, reference_texts):\n",
    "#     text_embedding = get_bert_embedding(text)\n",
    "#     reference_embeddings = [get_bert_embedding(ref) for ref in reference_texts]\n",
    "#     similarities = [cosine_similarity(text_embedding, ref_emb)[0][0] for ref_emb in reference_embeddings]\n",
    "#     return max(similarities) > 0.8  # Adjusted threshold for relevance\n",
    "\n",
    "# # Load your dataframe (assuming df is already defined and has a 'text' column)\n",
    "# reference_texts = [\n",
    "#     \"지하철 파업으로 인한 운행 중단\",\n",
    "#     \"지하철 사고 발생\",\n",
    "#     \"지하철 연착 문제\",\n",
    "#     \"지하철 노선 변경\",\n",
    "#     \"지하철 운행 재개\",\n",
    "#     \"지하철 서비스 중단\",\n",
    "#     \"지하철 안전 점검\",\n",
    "#     \"지하철 충돌 사고\",\n",
    "#     \"지하철 승객 혼잡\",\n",
    "#     \"지하철 출근 시간 문제\",\n",
    "#     \"지하철 노조 파업\",\n",
    "#     \"지하철 운행 변경\",\n",
    "#     \"지하철 대체 교통\"\n",
    "# ]\n",
    "\n",
    "# # Check if the document is relevant\n",
    "# def is_relevant_document(title):\n",
    "#     return check_dependency_and_entities(title) or check_topics(title, reference_texts)\n",
    "\n",
    "# # Assuming df is already defined and has a 'title' column\n",
    "# df['is_relevant'] = df['title'].apply(is_relevant_document)\n",
    "# df = df[df['is_relevant']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15f30ea-2b8e-488f-9053-0b7750dfb4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f4300-7993-41cd-8bc8-29ea168b451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data.utils.file import generate_s3_path\n",
    "s3_path = generate_s3_path(today_datetime=today_datetime)\n",
    "df = pd.read_csv(s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63db6f3d-b71f-4a99-b3d9-74acf9382034",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d221da71-7df2-495e-baf2-f8f3c446f991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. title 추출\n",
    "df['indexed_title'] = df.index.astype(str) + \": \" + df['title']\n",
    "titles = df['indexed_title'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0082369d-50e6-4093-b859-fcae3d83f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Selection News 태스크 실행\n",
    "from data.tasks.news_selection_task import SelectionNewsTask\n",
    "task = SelectionNewsTask()\n",
    "indices = task.execute(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661e4707-19e0-4451-95c3-e7c1e27c8f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_indices = [i for i in indices if i < len(df)]\n",
    "selected_df = df.iloc[valid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c29b39c-7e39-4ee2-b77d-22e5a837932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd7542-4463-4544-bd39-c6cabe4defec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 파일 S3 업로드\n",
    "s3_path = generate_s3_path(prefix=\"news_data_selected\", today_datetime=today_datetime)\n",
    "selected_df.to_csv(s3_path, index=False)\n",
    "print(f\"*** Data saved to {s3_path} ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4980f146-0a76-4e49-9fcc-9e8a289e9817",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e04abe-0da2-4ee9-8ce5-4514188d1655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
